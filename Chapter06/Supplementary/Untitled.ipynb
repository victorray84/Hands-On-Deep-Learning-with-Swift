{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "import string \n",
    "import numpy as np\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text file with 600893 characters\n"
     ]
    }
   ],
   "source": [
    "SRC = \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
    "DST = \"/Users/joshua.newnham/Documents/Shared Playground Data/RNN_Char_Data/nietzsche.txt\"\n",
    "\n",
    "dl_path = get_file(fname=DST, origin=SRC)\n",
    "\n",
    "with open(dl_path, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Loaded text file with {} characters\".format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vocabulary (which we will turn into a one-hot encoding vector and used to encode the inputs and outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 84\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "\n",
    "print(\"Vocabulary size {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Æ', 'ä', 'æ', 'é', 'ë']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove character that are not deemed useful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered vocabulary size 79\n"
     ]
    }
   ],
   "source": [
    "vocab = [c for c in vocab if c in string.printable]\n",
    "\n",
    "print(\"Filtered vocabulary size {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add **unknown** token to our vocab (which will be used to replace anything we don't have) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_CHAR = \"|\"\n",
    "vocab.insert(0, UNKNOWN_CHAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.printable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add **padding** token to our vocab (which is used to *pad* out a sequence, if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PADDING_CHAR = '\\f'\n",
    "# vocab.insert(0, PADDING_CHAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lookup dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_2_char = {idx : char for idx, char in enumerate(vocab)}\n",
    "char_2_idx = {char : idx for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_char(c, char_2_idx):\n",
    "    if c not in char_2_idx:\n",
    "        c = UNKNOWN_CHAR\n",
    "    \n",
    "    encoding = char_2_idx[c]\n",
    "    vec = np.zeros((len(char_2_idx), ), dtype=np.int32)\n",
    "    vec[encoding] = 1 \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the whole dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([encode_char(c, char_2_idx) for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600893, 80)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create our training set (X, y); where X is our sequence length (that we feed into the model) and y represents the last character that follows from X (what we want our model to predict).  \n",
    "For example: \n",
    "\n",
    "**Sample 1:** X = \"The quick bro\" and y = \"w\"   \n",
    "**Sample 2:** X = \"he quick brow\" and y = \"n\"  \n",
    "**...** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (600867, 25, 80), Y shape (600867, 1, 80)\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 25\n",
    "STRIDE = 1 \n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(0, data.shape[0] - (SEQ_LEN + 1), STRIDE):\n",
    "    data_x = data[i:i + SEQ_LEN,:]\n",
    "    data_y = data[i+SEQ_LEN:i+SEQ_LEN+1,:]\n",
    "    \n",
    "    X.append(data_x)\n",
    "    Y.append(data_y)\n",
    "        \n",
    "X = np.stack(X)\n",
    "Y = np.stack(Y)\n",
    "\n",
    "print(\"X shape {}, Y shape {}\".format(X.shape, Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600867, 80)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = Y.reshape((Y.shape[0], Y.shape[-1]))\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split our data into a training and validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (480693, 25, 80) (120174, 80), Valid (120174, 25, 80) (120174, 80)\n"
     ]
    }
   ],
   "source": [
    "train_count = int(X.shape[0] * 0.8) \n",
    "valid_count = X.shape[0] - train_count\n",
    "\n",
    "train_X = X[:train_count]\n",
    "train_y = Y[:train_count]\n",
    "\n",
    "valid_X = X[train_count:]\n",
    "valid_y = Y[train_count:]\n",
    "\n",
    "print(\"Train {} {}, Valid {} {}\".format(train_X.shape, valid_y.shape, valid_X.shape, valid_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 256)               345088    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 80)                20560     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 80)                0         \n",
      "=================================================================\n",
      "Total params: 365,648\n",
      "Trainable params: 365,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(256, input_shape=(X.shape[1], X.shape[2]), activation=None))\n",
    "model.add(tf.keras.layers.Activation(activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(Y.shape[-1], activation=None))\n",
    "model.add(tf.keras.layers.Activation(activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120174 samples, validate on 120174 samples\n",
      "Epoch 1/1000\n",
      "120174/120174 [==============================] - 150s 1ms/step - loss: 2.6404 - val_loss: 2.1916\n",
      "Epoch 2/1000\n",
      "120174/120174 [==============================] - 152s 1ms/step - loss: 2.1505 - val_loss: 1.9563\n",
      "Epoch 3/1000\n",
      "120174/120174 [==============================] - 154s 1ms/step - loss: 1.9726 - val_loss: 1.8224\n",
      "Epoch 4/1000\n",
      "120174/120174 [==============================] - 152s 1ms/step - loss: 1.8389 - val_loss: 1.6903\n",
      "Epoch 5/1000\n",
      "120174/120174 [==============================] - 165s 1ms/step - loss: 1.7366 - val_loss: 1.5829\n",
      "Epoch 6/1000\n",
      "120174/120174 [==============================] - 163s 1ms/step - loss: 1.6522 - val_loss: 1.5133\n",
      "Epoch 7/1000\n",
      "120174/120174 [==============================] - 168s 1ms/step - loss: 1.5781 - val_loss: 1.4401\n",
      "Epoch 8/1000\n",
      "120174/120174 [==============================] - 174s 1ms/step - loss: 1.5195 - val_loss: 1.3717\n",
      "Epoch 9/1000\n",
      "120174/120174 [==============================] - 171s 1ms/step - loss: 1.4686 - val_loss: 1.3099\n",
      "Epoch 10/1000\n",
      "120174/120174 [==============================] - 165s 1ms/step - loss: 1.4210 - val_loss: 1.2724\n",
      "Epoch 11/1000\n",
      "120174/120174 [==============================] - 163s 1ms/step - loss: 1.3809 - val_loss: 1.2425\n",
      "Epoch 12/1000\n",
      "120174/120174 [==============================] - 158s 1ms/step - loss: 1.3458 - val_loss: 1.1976\n",
      "Epoch 13/1000\n",
      "120174/120174 [==============================] - 160s 1ms/step - loss: 1.3104 - val_loss: 1.1583\n",
      "Epoch 14/1000\n",
      "120174/120174 [==============================] - 155s 1ms/step - loss: 1.2802 - val_loss: 1.1177\n",
      "Epoch 15/1000\n",
      "120174/120174 [==============================] - 147s 1ms/step - loss: 1.2515 - val_loss: 1.1001\n",
      "Epoch 16/1000\n",
      "120174/120174 [==============================] - 161s 1ms/step - loss: 1.2240 - val_loss: 1.0597\n",
      "Epoch 17/1000\n",
      "120174/120174 [==============================] - 156s 1ms/step - loss: 1.1932 - val_loss: 1.0515\n",
      "Epoch 18/1000\n",
      "120174/120174 [==============================] - 164s 1ms/step - loss: 1.1695 - val_loss: 1.0052\n",
      "Epoch 19/1000\n",
      "120174/120174 [==============================] - 161s 1ms/step - loss: 1.1527 - val_loss: 1.0872\n",
      "Epoch 20/1000\n",
      "120174/120174 [==============================] - 157s 1ms/step - loss: 1.1094 - val_loss: 0.9245\n",
      "Epoch 21/1000\n",
      "120174/120174 [==============================] - 156s 1ms/step - loss: 1.0598 - val_loss: 0.9006\n",
      "Epoch 22/1000\n",
      "120174/120174 [==============================] - 156s 1ms/step - loss: 1.2495 - val_loss: 1.1762\n",
      "Epoch 23/1000\n",
      "120174/120174 [==============================] - 155s 1ms/step - loss: 1.2591 - val_loss: 1.1174\n",
      "Epoch 24/1000\n",
      "120174/120174 [==============================] - 154s 1ms/step - loss: 1.2205 - val_loss: 1.0584\n",
      "Epoch 25/1000\n",
      "120174/120174 [==============================] - 165s 1ms/step - loss: 1.4518 - val_loss: 1.2691\n",
      "Epoch 26/1000\n",
      "120174/120174 [==============================] - 153s 1ms/step - loss: 4.0691 - val_loss: 14.7936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb41223dd8>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'lstm_checkpoint.h5', \n",
    "    monitor='val_loss', \n",
    "    verbose=0, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=True, \n",
    "    mode='auto', \n",
    "    period=3)\n",
    "    \n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5)\n",
    "\n",
    "model.fit(\n",
    "    valid_X, valid_y, \n",
    "    batch_size=64, \n",
    "    validation_data=(valid_X, valid_y), \n",
    "    epochs=1000, \n",
    "    callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('lstm_256_20190118.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(text):\n",
    "    # Vectorise text \n",
    "    x = [encode_char(c, char_2_idx) for c in text]\n",
    "    # Apply left padding \n",
    "    if len(x) > SEQ_LEN:\n",
    "        x = x[:SEQ_LEN]\n",
    "    elif len(x) < SEQ_LEN:\n",
    "        diff = SEQ_LEN - len(x)         \n",
    "        padding = [np.zeros_like(x[0])] * diff \n",
    "        x = padding + x\n",
    "    \n",
    "    x = np.array(x) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = prepare_input(\"hello ther\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 80)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(text):\n",
    "    x = prepare_input(text) \n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    prediction = model.predict(x)\n",
    "    vec = prediction[0] \n",
    "    vocab_idx = vec.argmax()\n",
    "    c = idx_2_char[vocab_idx]\n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_word(\"th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
