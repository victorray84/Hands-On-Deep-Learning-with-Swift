{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/Users/joshua.newnham/Documents/Shared Playground Data/Sketches/preprocessed/'\n",
    "VALID_DIR = os.path.join(ROOT_DIR, \"valid\")\n",
    "TRAIN_DIR = os.path.join(ROOT_DIR, \"train\")\n",
    "WEIGHTS_FILE = \"sketch_classifier.h5\"\n",
    "\n",
    "TARGET_SIZE = (128,128)\n",
    "INPUT_SHAPE = (128,128,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files(full_path):\n",
    "    count = 0 \n",
    "    def is_image(file_path):\n",
    "        image_extensions = ['png', 'jpg', 'jpeg']\n",
    "        \n",
    "        for image_extension in image_extensions:\n",
    "            if image_extension in file_path.lower():\n",
    "                return True\n",
    "            \n",
    "        return False \n",
    "    \n",
    "    for d in os.listdir(full_path):\n",
    "        if not os.path.isdir(os.path.join(full_path, d)):\n",
    "            continue\n",
    "            \n",
    "        sub_full_path = os.path.join(full_path, d)\n",
    "        \n",
    "        for f in os.listdir(sub_full_path):\n",
    "            img_path = os.path.join(sub_full_path, f)\n",
    "            if os.path.isfile(img_path) and is_image(img_path):\n",
    "                count += 1\n",
    "            \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_FILES = count_files(TRAIN_DIR)\n",
    "NUM_VALID_FILES = count_files(VALID_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    \n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.)\n",
    "    \n",
    "    # create an iterator for the training data \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        color_mode='grayscale')\n",
    "    \n",
    "    # create an iterator for the validation data \n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        VALID_DIR,\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        color_mode='grayscale')\n",
    "    \n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(WEIGHTS_FILE, \n",
    "                                           monitor='val_loss', \n",
    "                                           verbose=0, \n",
    "                                           save_best_only=True, \n",
    "                                           save_weights_only=True, \n",
    "                                           mode='auto', \n",
    "                                           period=2)\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=int(NUM_TRAIN_FILES/BATCH_SIZE),\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=int(NUM_VALID_FILES/BATCH_SIZE), \n",
    "        callbacks=[checkpoint, early_stopping]) \n",
    "    \n",
    "    return history, model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model):\n",
    "    \n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.)\n",
    "    \n",
    "    # create an iterator for the validation data \n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        VALID_DIR,\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        color_mode='grayscale')\n",
    "    \n",
    "    score = model.evaluate_generator(validation_generator)\n",
    "    \n",
    "    return score "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    65% accuracy input size 68\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3), \n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE, \n",
    "                                     name=\"l1_conv\"))    \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l1_relu\"))    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3), \n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     name=\"l1_conv_b\"))    \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l1_relu_b\"))    \n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, name=\"l1_maxpool\", padding='same'))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None, \n",
    "                                     name=\"l2_conv\"))            \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l2_relu\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, name=\"l2_maxpool\", padding='same')) \n",
    "    model.add(tf.keras.layers.Dropout(0.35, name=\"l3_dropout\"))\n",
    "        \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(129, \n",
    "                                    activation=None, \n",
    "                                    name='l4_fc'))    \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l4_relu\"))\n",
    "    model.add(tf.keras.layers.Dropout(0.35, name=\"l4_dropout\"))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, \n",
    "                                    activation=None, \n",
    "                                    name='l5_fc'))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "history, model = train(create_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    65% accuracy input size 68\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3), \n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE, \n",
    "                                     name=\"l1_conv\"))    \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l1_relu\"))    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3), \n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     name=\"l1_conv_b\"))    \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l1_relu_b\"))    \n",
    "    model.add(tf.keras.layers.MaxPooling2D(3,3, name=\"l1_maxpool\", padding='same'))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None, \n",
    "                                     name=\"l2_conv\"))            \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l2_relu\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, name=\"l2_maxpool\", padding='same')) \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None, \n",
    "                                     name=\"l3_conv\"))        \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l3_relu\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, name=\"l3_maxpool\", padding='same')) \n",
    "    model.add(tf.keras.layers.Dropout(0.4, name=\"l3_dropout\"))\n",
    "        \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(512, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(256, \n",
    "                                    activation=None, \n",
    "                                    name='l4_fc'))    \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l4_relu\"))\n",
    "    model.add(tf.keras.layers.Dropout(0.4, name=\"l4_dropout\"))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, \n",
    "                                    activation=None, \n",
    "                                    name='l5_fc'))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "450/450 [==============================] - 25s 57ms/step - loss: 2.5642 - acc: 0.2110 - val_loss: 1.9611 - val_acc: 0.3864\n",
      "Epoch 2/1000\n",
      "450/450 [==============================] - 24s 54ms/step - loss: 2.0212 - acc: 0.3814 - val_loss: 1.8084 - val_acc: 0.4943\n",
      "Epoch 3/1000\n",
      "450/450 [==============================] - 23s 51ms/step - loss: 1.7381 - acc: 0.4638 - val_loss: 1.6036 - val_acc: 0.5057\n",
      "Epoch 4/1000\n",
      "450/450 [==============================] - 22s 48ms/step - loss: 1.5279 - acc: 0.5251 - val_loss: 1.5731 - val_acc: 0.5341\n",
      "Epoch 5/1000\n",
      "450/450 [==============================] - 25s 55ms/step - loss: 1.3486 - acc: 0.5730 - val_loss: 1.4252 - val_acc: 0.6023\n",
      "Epoch 6/1000\n",
      "450/450 [==============================] - 20s 45ms/step - loss: 1.2041 - acc: 0.6149 - val_loss: 1.4535 - val_acc: 0.6250\n",
      "Epoch 7/1000\n",
      "450/450 [==============================] - 26s 57ms/step - loss: 1.0633 - acc: 0.6630 - val_loss: 1.3939 - val_acc: 0.6420\n",
      "Epoch 8/1000\n",
      "450/450 [==============================] - 21s 47ms/step - loss: 0.9626 - acc: 0.6897 - val_loss: 1.4401 - val_acc: 0.6136\n",
      "Epoch 9/1000\n",
      "450/450 [==============================] - 22s 49ms/step - loss: 0.8740 - acc: 0.7181 - val_loss: 1.4748 - val_acc: 0.6477\n",
      "Epoch 10/1000\n",
      "450/450 [==============================] - 24s 54ms/step - loss: 0.7752 - acc: 0.7473 - val_loss: 1.5589 - val_acc: 0.6534\n",
      "Epoch 11/1000\n",
      "450/450 [==============================] - 20s 45ms/step - loss: 0.7408 - acc: 0.7631 - val_loss: 1.5088 - val_acc: 0.6193\n",
      "Epoch 12/1000\n",
      "450/450 [==============================] - 26s 57ms/step - loss: 0.6400 - acc: 0.7963 - val_loss: 1.5044 - val_acc: 0.6591\n",
      "Epoch 13/1000\n",
      "450/450 [==============================] - 25s 57ms/step - loss: 0.6011 - acc: 0.8079 - val_loss: 1.7243 - val_acc: 0.6193\n",
      "Epoch 14/1000\n",
      "450/450 [==============================] - 22s 50ms/step - loss: 0.5312 - acc: 0.8183 - val_loss: 1.7458 - val_acc: 0.6591\n",
      "Epoch 15/1000\n",
      "450/450 [==============================] - 22s 48ms/step - loss: 0.5142 - acc: 0.8364 - val_loss: 1.6599 - val_acc: 0.6250\n",
      "Epoch 16/1000\n",
      "450/450 [==============================] - 22s 50ms/step - loss: 0.4682 - acc: 0.8500 - val_loss: 1.7221 - val_acc: 0.6193\n",
      "Epoch 17/1000\n",
      "450/450 [==============================] - 20s 45ms/step - loss: 0.4212 - acc: 0.8668 - val_loss: 1.8839 - val_acc: 0.6250\n",
      "Epoch 18/1000\n",
      "450/450 [==============================] - 22s 49ms/step - loss: 0.3967 - acc: 0.8675 - val_loss: 2.6238 - val_acc: 0.6477\n",
      "Epoch 19/1000\n",
      "450/450 [==============================] - 23s 50ms/step - loss: 0.3996 - acc: 0.8719 - val_loss: 1.8465 - val_acc: 0.6420\n",
      "Epoch 20/1000\n",
      "450/450 [==============================] - 21s 47ms/step - loss: 0.3650 - acc: 0.8912 - val_loss: 1.6101 - val_acc: 0.6193\n",
      "Epoch 21/1000\n",
      "450/450 [==============================] - 20s 44ms/step - loss: 0.3122 - acc: 0.9049 - val_loss: 1.9622 - val_acc: 0.6307\n",
      "Epoch 22/1000\n",
      "450/450 [==============================] - 23s 51ms/step - loss: 0.3213 - acc: 0.9026 - val_loss: 1.6187 - val_acc: 0.6648\n",
      "Epoch 23/1000\n",
      "450/450 [==============================] - 24s 53ms/step - loss: 0.3170 - acc: 0.9031 - val_loss: 2.0164 - val_acc: 0.6193\n",
      "Epoch 24/1000\n",
      "450/450 [==============================] - 23s 52ms/step - loss: 0.3003 - acc: 0.9033 - val_loss: 1.5438 - val_acc: 0.6193\n",
      "Epoch 25/1000\n",
      "450/450 [==============================] - 21s 47ms/step - loss: 0.3114 - acc: 0.9090 - val_loss: 2.1012 - val_acc: 0.6420\n",
      "Epoch 26/1000\n",
      "450/450 [==============================] - 26s 57ms/step - loss: 0.2935 - acc: 0.9142 - val_loss: 1.7437 - val_acc: 0.6307\n",
      "Epoch 27/1000\n",
      "450/450 [==============================] - 21s 46ms/step - loss: 0.2855 - acc: 0.9175 - val_loss: 2.0328 - val_acc: 0.6307\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_model():\n",
    "    inputs = tf.keras.layers.Input(shape=INPUT_SHAPE)    \n",
    "    x = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    stacks = []\n",
    "    for kernel_size in [7, 5, 3]:                \n",
    "        x0 = tf.keras.layers.Conv2D(\n",
    "            32, \n",
    "            kernel_size=(kernel_size,kernel_size), \n",
    "            strides=(1,1),  \n",
    "            padding='same',                                       \n",
    "            activation=None)(x)    \n",
    "        x0 = tf.keras.layers.Activation('relu')(x0)\n",
    "        stacks.append(x0)\n",
    "        \n",
    "    x = tf.keras.layers.concatenate(stacks,axis=-1)\n",
    "    x = tf.keras.layers.MaxPooling2D(2,2, padding='valid')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    \n",
    "    stacks = []\n",
    "    for kernel_size in [5, 3]:                \n",
    "        x0 = tf.keras.layers.Conv2D(\n",
    "            64, \n",
    "            kernel_size=(kernel_size,kernel_size), \n",
    "            strides=(1,1),  \n",
    "            padding='same',                                       \n",
    "            activation=None)(x)    \n",
    "        x0 = tf.keras.layers.Activation('relu')(x0)\n",
    "        stacks.append(x0)\n",
    "        \n",
    "    x = tf.keras.layers.concatenate(stacks,axis=-1)\n",
    "    x = tf.keras.layers.MaxPooling2D(2,2, padding='valid')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        128, \n",
    "        kernel_size=(3,3), \n",
    "        strides=(1,1),  \n",
    "        padding='same',                                       \n",
    "        activation=None)(x)    \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(2,2, padding='valid')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        \n",
    "    x = tf.keras.layers.Flatten()(x) \n",
    "    \n",
    "    x = tf.keras.layers.Dense(256, activation=None)(x) \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES, activation=None)(x) \n",
    "    output = tf.keras.layers.Activation('softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[output])\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "225/225 [==============================] - 78s 348ms/step - loss: 2.8129 - acc: 0.1676 - val_loss: 2.8365 - val_acc: 0.3636\n",
      "Epoch 2/1000\n",
      "225/225 [==============================] - 76s 340ms/step - loss: 2.2693 - acc: 0.3150 - val_loss: 2.1543 - val_acc: 0.4602\n",
      "Epoch 3/1000\n",
      "225/225 [==============================] - 79s 351ms/step - loss: 2.0154 - acc: 0.3829 - val_loss: 1.8296 - val_acc: 0.5114\n",
      "Epoch 4/1000\n",
      "225/225 [==============================] - 81s 360ms/step - loss: 1.7968 - acc: 0.4433 - val_loss: 1.5984 - val_acc: 0.5625\n",
      "Epoch 5/1000\n",
      "225/225 [==============================] - 82s 366ms/step - loss: 1.6842 - acc: 0.4804 - val_loss: 1.4696 - val_acc: 0.5909\n",
      "Epoch 6/1000\n",
      "225/225 [==============================] - 82s 363ms/step - loss: 1.5768 - acc: 0.5103 - val_loss: 1.4825 - val_acc: 0.5511\n",
      "Epoch 7/1000\n",
      "225/225 [==============================] - 80s 357ms/step - loss: 1.4380 - acc: 0.5434 - val_loss: 1.4429 - val_acc: 0.5909\n",
      "Epoch 8/1000\n",
      "225/225 [==============================] - 76s 338ms/step - loss: 1.3470 - acc: 0.5707 - val_loss: 1.3643 - val_acc: 0.6136\n",
      "Epoch 9/1000\n",
      "225/225 [==============================] - 76s 339ms/step - loss: 1.2521 - acc: 0.6007 - val_loss: 1.3303 - val_acc: 0.6136\n",
      "Epoch 10/1000\n",
      "225/225 [==============================] - 76s 338ms/step - loss: 1.1692 - acc: 0.6202 - val_loss: 1.3029 - val_acc: 0.6307\n",
      "Epoch 11/1000\n",
      "225/225 [==============================] - 77s 341ms/step - loss: 1.0975 - acc: 0.6502 - val_loss: 1.3553 - val_acc: 0.6080\n",
      "Epoch 12/1000\n",
      "225/225 [==============================] - 76s 339ms/step - loss: 1.0586 - acc: 0.6557 - val_loss: 1.3069 - val_acc: 0.6080\n",
      "Epoch 13/1000\n",
      "225/225 [==============================] - 76s 337ms/step - loss: 1.0183 - acc: 0.6676 - val_loss: 1.2870 - val_acc: 0.6193\n",
      "Epoch 14/1000\n",
      "225/225 [==============================] - 76s 338ms/step - loss: 0.9451 - acc: 0.6825 - val_loss: 1.3567 - val_acc: 0.6080\n",
      "Epoch 15/1000\n",
      "225/225 [==============================] - 75s 334ms/step - loss: 0.9224 - acc: 0.6878 - val_loss: 1.3120 - val_acc: 0.6080\n",
      "Epoch 16/1000\n",
      "225/225 [==============================] - 75s 335ms/step - loss: 0.8670 - acc: 0.7213 - val_loss: 1.3650 - val_acc: 0.6364\n",
      "Epoch 17/1000\n",
      "225/225 [==============================] - 76s 336ms/step - loss: 0.8352 - acc: 0.7286 - val_loss: 1.4855 - val_acc: 0.6136\n",
      "Epoch 18/1000\n",
      "225/225 [==============================] - 76s 336ms/step - loss: 0.8218 - acc: 0.7319 - val_loss: 1.2980 - val_acc: 0.6136\n",
      "Epoch 19/1000\n",
      "225/225 [==============================] - 76s 338ms/step - loss: 0.7911 - acc: 0.7389 - val_loss: 1.3574 - val_acc: 0.6136\n",
      "Epoch 20/1000\n",
      "225/225 [==============================] - 78s 345ms/step - loss: 0.7218 - acc: 0.7525 - val_loss: 1.4707 - val_acc: 0.6364\n",
      "Epoch 21/1000\n",
      "225/225 [==============================] - 77s 341ms/step - loss: 0.7293 - acc: 0.7630 - val_loss: 1.3950 - val_acc: 0.6364\n",
      "Epoch 22/1000\n",
      "225/225 [==============================] - 75s 335ms/step - loss: 0.7145 - acc: 0.7616 - val_loss: 1.3859 - val_acc: 0.6136\n",
      "Epoch 23/1000\n",
      "225/225 [==============================] - 73s 327ms/step - loss: 0.6715 - acc: 0.7792 - val_loss: 1.4122 - val_acc: 0.6023\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential() \n",
    "    model.add(tf.keras.layers.Conv2D(16, kernel_size=(7,7), strides=(3,3),  \n",
    "                            padding='same', activation='relu', input_shape=INPUT_SHAPE))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, padding='same'))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(5,5), padding='same', activation='relu'))        \n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, padding='same'))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(64, (5,5), padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, padding='same'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.125))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(128, (5,5), padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())                \n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu', name='dense_2_512'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax', name='output'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer='rmsprop', \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 2.8709 - acc: 0.1103 - val_loss: 2.4940 - val_acc: 0.1989\n",
      "Epoch 2/1000\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 2.3486 - acc: 0.2643 - val_loss: 2.1257 - val_acc: 0.3239\n",
      "Epoch 3/1000\n",
      "225/225 [==============================] - 4s 20ms/step - loss: 2.0257 - acc: 0.3671 - val_loss: 2.0584 - val_acc: 0.3864\n",
      "Epoch 4/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 1.8254 - acc: 0.4257 - val_loss: 1.8259 - val_acc: 0.4943\n",
      "Epoch 5/1000\n",
      "225/225 [==============================] - 4s 20ms/step - loss: 1.6348 - acc: 0.4779 - val_loss: 1.7840 - val_acc: 0.4830\n",
      "Epoch 6/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 1.4563 - acc: 0.5352 - val_loss: 1.7792 - val_acc: 0.4773\n",
      "Epoch 7/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 1.3071 - acc: 0.5807 - val_loss: 1.8117 - val_acc: 0.5057\n",
      "Epoch 8/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 1.1571 - acc: 0.6213 - val_loss: 1.8888 - val_acc: 0.5455\n",
      "Epoch 9/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.9867 - acc: 0.6719 - val_loss: 1.7259 - val_acc: 0.5625\n",
      "Epoch 10/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.8641 - acc: 0.7061 - val_loss: 1.9153 - val_acc: 0.5341\n",
      "Epoch 11/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.7429 - acc: 0.7611 - val_loss: 1.8997 - val_acc: 0.5795\n",
      "Epoch 12/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.6109 - acc: 0.7950 - val_loss: 1.8679 - val_acc: 0.5795\n",
      "Epoch 13/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.5055 - acc: 0.8250 - val_loss: 2.2561 - val_acc: 0.5284\n",
      "Epoch 14/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.4134 - acc: 0.8581 - val_loss: 2.3952 - val_acc: 0.5455\n",
      "Epoch 15/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.3640 - acc: 0.8744 - val_loss: 2.5897 - val_acc: 0.5511\n",
      "Epoch 16/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.2891 - acc: 0.9078 - val_loss: 2.4962 - val_acc: 0.5795\n",
      "Epoch 17/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.2361 - acc: 0.9189 - val_loss: 2.8124 - val_acc: 0.5341\n",
      "Epoch 18/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.2125 - acc: 0.9281 - val_loss: 2.8239 - val_acc: 0.5511\n",
      "Epoch 19/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.1861 - acc: 0.9408 - val_loss: 2.8668 - val_acc: 0.5511\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential() \n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), strides=(3,3),  \n",
    "                            padding='same', activation='relu', input_shape=INPUT_SHAPE))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), padding='same', activation='relu'))        \n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2))      \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), padding='same', activation='relu'))        \n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2))   \n",
    "    model.add(tf.keras.layers.Dropout(0.33))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())                \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu', name='dense_2_512'))\n",
    "    model.add(tf.keras.layers.Dropout(0.33))\n",
    "    \n",
    "#     model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dropout(0.33))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax', name='output'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer='rmsprop', \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 2.9253 - acc: 0.1126 - val_loss: 2.4768 - val_acc: 0.2500\n",
      "Epoch 2/1000\n",
      "225/225 [==============================] - 4s 16ms/step - loss: 2.4946 - acc: 0.2264 - val_loss: 2.1238 - val_acc: 0.3750\n",
      "Epoch 3/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 2.2695 - acc: 0.2923 - val_loss: 1.9798 - val_acc: 0.3580\n",
      "Epoch 4/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 2.1537 - acc: 0.3220 - val_loss: 1.9027 - val_acc: 0.4545\n",
      "Epoch 5/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 2.0305 - acc: 0.3713 - val_loss: 1.8102 - val_acc: 0.4261\n",
      "Epoch 6/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.9485 - acc: 0.3893 - val_loss: 1.7647 - val_acc: 0.4432\n",
      "Epoch 7/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.8978 - acc: 0.4000 - val_loss: 1.7754 - val_acc: 0.4432\n",
      "Epoch 8/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.8175 - acc: 0.4317 - val_loss: 1.7484 - val_acc: 0.4489\n",
      "Epoch 9/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.7587 - acc: 0.4427 - val_loss: 1.7526 - val_acc: 0.4318\n",
      "Epoch 10/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.7169 - acc: 0.4585 - val_loss: 1.7008 - val_acc: 0.5000\n",
      "Epoch 11/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.6646 - acc: 0.4758 - val_loss: 1.7251 - val_acc: 0.4659\n",
      "Epoch 12/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.6057 - acc: 0.4969 - val_loss: 1.7137 - val_acc: 0.5341\n",
      "Epoch 13/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.5543 - acc: 0.5065 - val_loss: 1.6717 - val_acc: 0.4943\n",
      "Epoch 14/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.5248 - acc: 0.5221 - val_loss: 1.6972 - val_acc: 0.5000\n",
      "Epoch 15/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.4714 - acc: 0.5194 - val_loss: 1.6988 - val_acc: 0.4886\n",
      "Epoch 16/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.4328 - acc: 0.5337 - val_loss: 1.7379 - val_acc: 0.4830\n",
      "Epoch 17/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.3710 - acc: 0.5568 - val_loss: 1.7341 - val_acc: 0.5227\n",
      "Epoch 18/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.3304 - acc: 0.5736 - val_loss: 1.6625 - val_acc: 0.5511\n",
      "Epoch 19/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.3054 - acc: 0.5772 - val_loss: 1.6976 - val_acc: 0.5284\n",
      "Epoch 20/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.2594 - acc: 0.5854 - val_loss: 1.6964 - val_acc: 0.5057\n",
      "Epoch 21/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.2401 - acc: 0.5991 - val_loss: 1.6818 - val_acc: 0.5398\n",
      "Epoch 22/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.2161 - acc: 0.6163 - val_loss: 1.7270 - val_acc: 0.5284\n",
      "Epoch 23/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.1632 - acc: 0.6164 - val_loss: 1.7216 - val_acc: 0.5227\n",
      "Epoch 24/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.1608 - acc: 0.6184 - val_loss: 1.7040 - val_acc: 0.5511\n",
      "Epoch 25/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.1120 - acc: 0.6306 - val_loss: 1.7159 - val_acc: 0.5625\n",
      "Epoch 26/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.0617 - acc: 0.6556 - val_loss: 1.8203 - val_acc: 0.5284\n",
      "Epoch 27/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.0647 - acc: 0.6502 - val_loss: 1.8528 - val_acc: 0.5398\n",
      "Epoch 28/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.0606 - acc: 0.6471 - val_loss: 1.8195 - val_acc: 0.5568\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential() \n",
    "    model.add(tf.keras.layers.Conv2D(16, kernel_size=(5,5), strides=(3,3),  \n",
    "                            padding='same', activation='relu', input_shape=INPUT_SHAPE))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, padding='same'))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), padding='same', activation='relu'))        \n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, padding='same'))   \n",
    "    model.add(tf.keras.layers.Dropout(0.33))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())                \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu', name='dense_2_512'))\n",
    "    model.add(tf.keras.layers.Dropout(0.33))\n",
    "    \n",
    "#     model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dropout(0.33))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax', name='output'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer='rmsprop', \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "225/225 [==============================] - 4s 20ms/step - loss: 2.6679 - acc: 0.1829 - val_loss: 2.1610 - val_acc: 0.3239\n",
      "Epoch 2/1000\n",
      "225/225 [==============================] - 4s 17ms/step - loss: 2.2071 - acc: 0.3184 - val_loss: 1.8687 - val_acc: 0.4602\n",
      "Epoch 3/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.9675 - acc: 0.3945 - val_loss: 1.7776 - val_acc: 0.4659\n",
      "Epoch 4/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.8038 - acc: 0.4513 - val_loss: 1.7342 - val_acc: 0.4602\n",
      "Epoch 5/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.6667 - acc: 0.4898 - val_loss: 1.6631 - val_acc: 0.5114\n",
      "Epoch 6/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.5413 - acc: 0.5140 - val_loss: 1.6376 - val_acc: 0.5057\n",
      "Epoch 7/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.4631 - acc: 0.5408 - val_loss: 1.6109 - val_acc: 0.5284\n",
      "Epoch 8/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.3879 - acc: 0.5593 - val_loss: 1.6443 - val_acc: 0.5227\n",
      "Epoch 9/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.3095 - acc: 0.5912 - val_loss: 1.6369 - val_acc: 0.5341\n",
      "Epoch 10/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.2117 - acc: 0.6142 - val_loss: 1.6055 - val_acc: 0.5057\n",
      "Epoch 11/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.1446 - acc: 0.6259 - val_loss: 1.6319 - val_acc: 0.5284\n",
      "Epoch 12/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.0691 - acc: 0.6554 - val_loss: 1.6432 - val_acc: 0.5398\n",
      "Epoch 13/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.9891 - acc: 0.6824 - val_loss: 1.6752 - val_acc: 0.5625\n",
      "Epoch 14/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.9368 - acc: 0.7008 - val_loss: 1.6640 - val_acc: 0.5284\n",
      "Epoch 15/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.8868 - acc: 0.7123 - val_loss: 1.6634 - val_acc: 0.5568\n",
      "Epoch 16/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 0.8329 - acc: 0.7290 - val_loss: 1.6740 - val_acc: 0.5511\n",
      "Epoch 17/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.7610 - acc: 0.7459 - val_loss: 1.7718 - val_acc: 0.5000\n",
      "Epoch 18/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 0.7178 - acc: 0.7644 - val_loss: 1.7786 - val_acc: 0.5455\n",
      "Epoch 19/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 0.7077 - acc: 0.7641 - val_loss: 1.7568 - val_acc: 0.5455\n",
      "Epoch 20/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.6554 - acc: 0.7861 - val_loss: 1.9291 - val_acc: 0.5227\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.)\n",
    "    \n",
    "# create an iterator for the training data \n",
    "train_generator = datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=NUM_TRAIN_FILES, \n",
    "    color_mode='grayscale')\n",
    "\n",
    "# create an iterator for the validation data \n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    VALID_DIR,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=NUM_VALID_FILES, \n",
    "    color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_generator.next()\n",
    "x_test, y_test = validation_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 99.67%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 65.91%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export weights\n",
    "The type of each entry in array is given by -dataType. The number of entries is equal to:\n",
    "\n",
    "inputFeatureChannels outputFeatureChannels kernelHeight kernelWidth*\n",
    "\n",
    "The layout of filter weight is as a 4D tensor (array) weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ inputChannels / groups ]\n",
    "\n",
    "Note: For binary-convolutions the layout of the weights are: weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ floor((inputChannels/groups)+31) / 32 ] with each 32 sub input feature channel index specified in machine byte order, so that for example the 13th feature channel bit can be extracted using bitmask = (1U << 13)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_conv_weights(name, wts_coef, bias_coef):\n",
    "    print(\"Exporting weights for {}\\n\\t{}\\n\\t{}\".format(name, \n",
    "          os.path.join('exports', \"{}_conv.data\".format(name)), \n",
    "          os.path.join('exports', \"{}_bias.data\".format(name))))\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # [kernel_width, kernel_height, input_feature_channels, output_feature_channels]\n",
    "    print(\"\\tOriginal weights shape {}\".format(wts_coef.shape))\n",
    "    if bias_coef is not None:\n",
    "        # [output_feature_channels]\n",
    "        print(\"\\tOriginal bias shape {}\".format(bias_coef.shape))\n",
    "    \n",
    "    # [output_feature_channels, kernel_width, kernel_height, input_feature_channels]\n",
    "    wts_coef = wts_coef.transpose(3, 0, 1, 2)\n",
    "    print(\"\\tReshaped weights shape {}\".format(wts_coef.shape))    \n",
    "    wts_coef.tofile(os.path.join('exports', \"{}_conv.data\".format(name)))    \n",
    "    \n",
    "    if bias_coef is not None:\n",
    "        bias_coef = np.squeeze(bias_coef)\n",
    "        print(\"\\tReshaped bias_coef shape {}\".format(bias_coef.shape))    \n",
    "        bias_coef.tofile(os.path.join('exports', \"{}_bias.data\".format(name)))    \n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dense_weights(name, wts_coef, bias_coef, kernel_width, kernel_height):\n",
    "    \"\"\"\n",
    "    A fully connected layer in a Convolutional Neural Network (CNN) is one where every input channel is connected \n",
    "    to every output channel. The kernel width is equal to the width of the source image, and the \n",
    "    kernel height is equal to the height of the source image. The width and height of the output is 1 x 1.\n",
    "    \n",
    "    A fully connected layer takes an MPSImage object with dimensions \n",
    "    source.width x source.height x Ni, convolves it with Weights[No][source.width][source.height][Ni], \n",
    "    and produces a 1 x 1 x No output.\n",
    "    \n",
    "    Thus, the following conditions must be true:\n",
    "    - kernelWidth == source.width\n",
    "    - kernelHeight == source.height\n",
    "    - clipRect.size.width == 1\n",
    "    - clipRect.size.height == 1\n",
    "    \n",
    "    You can think of a fully connected layer as a matrix multiplication where the image is \n",
    "    flattened into a vector of length source.width*source.height*Ni, and the weights are arranged in a \n",
    "    matrix of dimension No x (source.width*source.height*Ni) to produce an output vector of length No.\n",
    "    \n",
    "    The value of the strideInPixelsX, strideInPixelsY, and groups properties must be 1. \n",
    "    The offset property is not applicable and it is ignored. Because the clip rectangle is \n",
    "    clamped to the destination image bounds, if the destination is 1 x 1, you do not need to set the \n",
    "    clipRect property.\n",
    "    \"\"\"\n",
    "    print(\"Exporting weights for {}\\n\\t{}\\n\\t{}\".format(name, \n",
    "          os.path.join('exports', \"{}_conv.data\".format(name)), \n",
    "          os.path.join('exports', \"{}_bias.data\".format(name))))\n",
    "        \n",
    "    input_feature_channels = int(wts_coef.shape[0] / kernel_width / kernel_height) \n",
    "    output_feature_channels = wts_coef.shape[-1]            \n",
    "    \n",
    "    # [kernel_width, kernel_height, input_feature_channels, output_feature_channels]\n",
    "    print(\"\\tOriginal weights shape {}\".format(wts_coef.shape))\n",
    "    \n",
    "    #wts_coef = np.reshape(wts_coef, [kernel_width, kernel_height, input_feature_channels, output_feature_channels])    \n",
    "    wts_coef = np.reshape(wts_coef, [kernel_width, kernel_height, -1, output_feature_channels])    \n",
    "        \n",
    "    if bias_coef is not None:\n",
    "        # [output_feature_channels]\n",
    "        print(\"\\tOriginal bias shape {}\".format(bias_coef.shape))\n",
    "    \n",
    "    # [output_feature_channels, kernel_width, kernel_height, input_feature_channels]\n",
    "    wts_coef = wts_coef.transpose(3, 0, 1, 2)\n",
    "    print(\"\\tReshaped weights shape {}\".format(wts_coef.shape))    \n",
    "    wts_coef.tofile(os.path.join('exports', \"{}_conv.data\".format(name)))    \n",
    "    \n",
    "    if bias_coef is not None:\n",
    "        bias_coef = np.squeeze(bias_coef)\n",
    "        print(\"\\tReshaped bias_coef shape {}\".format(bias_coef.shape))    \n",
    "        bias_coef.tofile(os.path.join('exports', \"{}_bias.data\".format(name)))   \n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatted_input_kernel_width = None\n",
    "flatted_input_kernel_height = None\n",
    "\n",
    "for layer in model.layers:        \n",
    "    if \"flatten\" in layer.name:\n",
    "        flatted_input_kernel_width = layer.input_shape[1] # None, 14, 14, 64\n",
    "        flatted_input_kernel_height = layer.input_shape[2] # None, 14, 14, 64\n",
    "        print(\"flatted_input_kernel_width {} flatted_input_kernel_height {}\".format(\n",
    "            flatted_input_kernel_width, flatted_input_kernel_height))\n",
    "        \n",
    "    if len(layer.get_weights()) > 0:        \n",
    "        name = layer.name         \n",
    "        wts = layer.get_weights()\n",
    "        \n",
    "        if \"conv\" in name:            \n",
    "            export_conv_weights(layer.name, wts[0], wts[1] if len(wts) == 2 else None)        \n",
    "        else:\n",
    "            export_dense_weights(layer.name, wts[0], wts[1] if len(wts) == 2 else None, \n",
    "                                flatted_input_kernel_width, flatted_input_kernel_height)        \n",
    "            # after the initial pass (from cnn to fcn); flattern the kernel down to 1x1 \n",
    "            # i.e. update the flatted_input_kernel_DIM to have the kernel width and height of 1 \n",
    "            flatted_input_kernel_width, flatted_input_kernel_height = 1, 1             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    https://www.iioab.org/articles/IIOABJ_7.S5_337-341.pdf\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(96, \n",
    "                                     kernel_size=(10,10), \n",
    "                                     strides=(1,1),  \n",
    "                                     padding='valid',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(192, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='valid', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "    model.add(tf.keras.layers.Dropout(0.35))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(192, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='valid', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "        \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(192, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.35))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES,activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=\"sgd\", # tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 55, 55, 96)        9696      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 55, 55, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 23, 23, 192)       460992    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 23, 23, 192)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 11, 11, 192)       0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 11, 11, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 9, 9, 192)         331968    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 9, 9, 192)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 15552)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 192)               2986176   \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 22)                4246      \n",
      "_________________________________________________________________\n",
      "output (Activation)          (None, 22)                0         \n",
      "=================================================================\n",
      "Total params: 3,793,078\n",
      "Trainable params: 3,793,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "225/225 [==============================] - 53s 235ms/step - loss: 3.0778 - acc: 0.0553 - val_loss: 3.0429 - val_acc: 0.1364\n",
      "Epoch 2/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 2.9464 - acc: 0.1145 - val_loss: 2.6928 - val_acc: 0.1989\n",
      "Epoch 3/1000\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 2.6573 - acc: 0.2001 - val_loss: 2.4005 - val_acc: 0.3011\n",
      "Epoch 4/1000\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 2.4546 - acc: 0.2484 - val_loss: 2.1870 - val_acc: 0.3693\n",
      "Epoch 5/1000\n",
      "225/225 [==============================] - 48s 212ms/step - loss: 2.3132 - acc: 0.2868 - val_loss: 2.0689 - val_acc: 0.3920\n",
      "Epoch 6/1000\n",
      "225/225 [==============================] - 48s 211ms/step - loss: 2.2147 - acc: 0.3164 - val_loss: 1.9824 - val_acc: 0.4148\n",
      "Epoch 7/1000\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 2.1061 - acc: 0.3528 - val_loss: 1.9519 - val_acc: 0.4091\n",
      "Epoch 8/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 2.0262 - acc: 0.3724 - val_loss: 1.8855 - val_acc: 0.4261\n",
      "Epoch 9/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 1.9546 - acc: 0.3989 - val_loss: 1.8288 - val_acc: 0.4489\n",
      "Epoch 10/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 1.8709 - acc: 0.4143 - val_loss: 1.8184 - val_acc: 0.4148\n",
      "Epoch 11/1000\n",
      "225/225 [==============================] - 50s 220ms/step - loss: 1.8135 - acc: 0.4392 - val_loss: 1.8040 - val_acc: 0.4716\n",
      "Epoch 12/1000\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 1.7353 - acc: 0.4593 - val_loss: 1.7699 - val_acc: 0.4886\n",
      "Epoch 13/1000\n",
      "225/225 [==============================] - 50s 223ms/step - loss: 1.6673 - acc: 0.4828 - val_loss: 1.7341 - val_acc: 0.4830\n",
      "Epoch 14/1000\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 1.6040 - acc: 0.4988 - val_loss: 1.7696 - val_acc: 0.4545\n",
      "Epoch 15/1000\n",
      "225/225 [==============================] - 52s 230ms/step - loss: 1.5424 - acc: 0.5283 - val_loss: 1.7596 - val_acc: 0.5000\n",
      "Epoch 16/1000\n",
      "225/225 [==============================] - 51s 226ms/step - loss: 1.4865 - acc: 0.5348 - val_loss: 1.7462 - val_acc: 0.5000\n",
      "Epoch 17/1000\n",
      "225/225 [==============================] - 50s 220ms/step - loss: 1.4114 - acc: 0.5507 - val_loss: 1.7562 - val_acc: 0.5114\n",
      "Epoch 18/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 1.3431 - acc: 0.5774 - val_loss: 1.7774 - val_acc: 0.5114\n",
      "Epoch 19/1000\n",
      "225/225 [==============================] - 50s 220ms/step - loss: 1.3393 - acc: 0.5755 - val_loss: 1.7598 - val_acc: 0.5227\n",
      "Epoch 20/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 1.2432 - acc: 0.6048 - val_loss: 1.7609 - val_acc: 0.4830\n",
      "Epoch 21/1000\n",
      "225/225 [==============================] - 51s 226ms/step - loss: 1.2027 - acc: 0.6193 - val_loss: 1.8069 - val_acc: 0.4830\n",
      "Epoch 22/1000\n",
      "225/225 [==============================] - 50s 224ms/step - loss: 1.1417 - acc: 0.6328 - val_loss: 1.8277 - val_acc: 0.5000\n",
      "Epoch 23/1000\n",
      "225/225 [==============================] - 51s 228ms/step - loss: 1.0933 - acc: 0.6499 - val_loss: 1.7890 - val_acc: 0.5341\n"
     ]
    }
   ],
   "source": [
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(7,7), \n",
    "                                     strides=(2,2),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))           \n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(3,3))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))        \n",
    "    model.add(tf.keras.layers.Dropout(0.4)) \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(32, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.3))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_63 (Conv2D)           (None, 64, 64, 32)        1600      \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_64 (Conv2D)           (None, 64, 64, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_65 (Conv2D)           (None, 21, 21, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_66 (Conv2D)           (None, 10, 10, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 32)                25632     \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 22)                726       \n",
      "_________________________________________________________________\n",
      "output (Activation)          (None, 22)                0         \n",
      "=================================================================\n",
      "Total params: 88,470\n",
      "Trainable params: 88,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "450/450 [==============================] - 60s 133ms/step - loss: 2.9245 - acc: 0.1098 - val_loss: 2.4482 - val_acc: 0.2784\n",
      "Epoch 2/1000\n",
      "450/450 [==============================] - 59s 131ms/step - loss: 2.4531 - acc: 0.2420 - val_loss: 2.1753 - val_acc: 0.4034\n",
      "Epoch 3/1000\n",
      "450/450 [==============================] - 57s 126ms/step - loss: 2.2052 - acc: 0.3226 - val_loss: 1.9569 - val_acc: 0.4716\n",
      "Epoch 4/1000\n",
      "450/450 [==============================] - 57s 128ms/step - loss: 2.0291 - acc: 0.3707 - val_loss: 1.6818 - val_acc: 0.5284\n",
      "Epoch 5/1000\n",
      "450/450 [==============================] - 57s 127ms/step - loss: 1.8622 - acc: 0.4229 - val_loss: 1.6938 - val_acc: 0.5227\n",
      "Epoch 6/1000\n",
      "450/450 [==============================] - 60s 133ms/step - loss: 1.7634 - acc: 0.4637 - val_loss: 1.6022 - val_acc: 0.5739\n",
      "Epoch 7/1000\n",
      "450/450 [==============================] - 58s 130ms/step - loss: 1.6562 - acc: 0.4853 - val_loss: 1.4138 - val_acc: 0.6250\n",
      "Epoch 8/1000\n",
      "450/450 [==============================] - 58s 129ms/step - loss: 1.6351 - acc: 0.4879 - val_loss: 1.4070 - val_acc: 0.6534\n",
      "Epoch 9/1000\n",
      "450/450 [==============================] - 58s 129ms/step - loss: 1.5212 - acc: 0.5211 - val_loss: 1.3859 - val_acc: 0.6477\n",
      "Epoch 10/1000\n",
      "450/450 [==============================] - 58s 129ms/step - loss: 1.4752 - acc: 0.5501 - val_loss: 1.2851 - val_acc: 0.6875\n",
      "Epoch 11/1000\n",
      "450/450 [==============================] - 58s 129ms/step - loss: 1.3827 - acc: 0.5676 - val_loss: 1.3190 - val_acc: 0.6534\n",
      "Epoch 12/1000\n",
      "450/450 [==============================] - 57s 128ms/step - loss: 1.3595 - acc: 0.5690 - val_loss: 1.2903 - val_acc: 0.6818\n",
      "Epoch 13/1000\n",
      "450/450 [==============================] - 59s 130ms/step - loss: 1.2975 - acc: 0.5824 - val_loss: 1.2893 - val_acc: 0.6477\n",
      "Epoch 14/1000\n",
      "450/450 [==============================] - 59s 132ms/step - loss: 1.2863 - acc: 0.5887 - val_loss: 1.2228 - val_acc: 0.6420\n",
      "Epoch 15/1000\n",
      "450/450 [==============================] - 57s 128ms/step - loss: 1.2355 - acc: 0.6122 - val_loss: 1.1436 - val_acc: 0.6534\n",
      "Epoch 16/1000\n",
      "450/450 [==============================] - 58s 129ms/step - loss: 1.2148 - acc: 0.6243 - val_loss: 1.2131 - val_acc: 0.6761\n",
      "Epoch 17/1000\n",
      "450/450 [==============================] - 59s 130ms/step - loss: 1.1982 - acc: 0.6188 - val_loss: 1.2372 - val_acc: 0.6477\n",
      "Epoch 18/1000\n",
      "450/450 [==============================] - 60s 133ms/step - loss: 1.1580 - acc: 0.6354 - val_loss: 1.1899 - val_acc: 0.6591\n",
      "Epoch 19/1000\n",
      "450/450 [==============================] - 58s 129ms/step - loss: 1.1299 - acc: 0.6393 - val_loss: 1.2563 - val_acc: 0.6420\n",
      "Epoch 20/1000\n",
      "450/450 [==============================] - 59s 131ms/step - loss: 1.1387 - acc: 0.6390 - val_loss: 1.2186 - val_acc: 0.6705\n"
     ]
    }
   ],
   "source": [
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (128,128)\n",
    "INPUT_SHAPE = (128,128,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(7,7), \n",
    "                                     strides=(2,2),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))           \n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))        \n",
    "    model.add(tf.keras.layers.Dropout(0.4)) \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(32, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.3))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "56/56 [==============================] - 53s 948ms/step - loss: 2.9346 - acc: 0.1220 - val_loss: 2.6917 - val_acc: 0.2500\n",
      "Epoch 2/1000\n",
      "56/56 [==============================] - 52s 929ms/step - loss: 2.5938 - acc: 0.2317 - val_loss: 2.2908 - val_acc: 0.3516\n",
      "Epoch 3/1000\n",
      "56/56 [==============================] - 51s 915ms/step - loss: 2.3669 - acc: 0.2942 - val_loss: 2.1253 - val_acc: 0.4609\n",
      "Epoch 4/1000\n",
      "56/56 [==============================] - 51s 919ms/step - loss: 2.1921 - acc: 0.3366 - val_loss: 1.9819 - val_acc: 0.4688\n",
      "Epoch 5/1000\n",
      "56/56 [==============================] - 51s 907ms/step - loss: 2.0813 - acc: 0.3659 - val_loss: 1.7306 - val_acc: 0.5312\n",
      "Epoch 6/1000\n",
      "56/56 [==============================] - 51s 919ms/step - loss: 1.9778 - acc: 0.4036 - val_loss: 1.5973 - val_acc: 0.5391\n",
      "Epoch 7/1000\n",
      "56/56 [==============================] - 51s 906ms/step - loss: 1.8926 - acc: 0.4161 - val_loss: 1.6025 - val_acc: 0.5625\n",
      "Epoch 8/1000\n",
      "56/56 [==============================] - 49s 879ms/step - loss: 1.7822 - acc: 0.4465 - val_loss: 1.5320 - val_acc: 0.5625\n",
      "Epoch 9/1000\n",
      "56/56 [==============================] - 52s 921ms/step - loss: 1.7252 - acc: 0.4671 - val_loss: 1.4967 - val_acc: 0.5703\n",
      "Epoch 10/1000\n",
      "56/56 [==============================] - 50s 901ms/step - loss: 1.6128 - acc: 0.5028 - val_loss: 1.4015 - val_acc: 0.6172\n",
      "Epoch 11/1000\n",
      "56/56 [==============================] - 51s 918ms/step - loss: 1.6494 - acc: 0.4945 - val_loss: 1.3893 - val_acc: 0.6016\n",
      "Epoch 12/1000\n",
      "56/56 [==============================] - 51s 914ms/step - loss: 1.5395 - acc: 0.5174 - val_loss: 1.3733 - val_acc: 0.6016\n",
      "Epoch 13/1000\n",
      "56/56 [==============================] - 50s 901ms/step - loss: 1.5177 - acc: 0.5225 - val_loss: 1.4943 - val_acc: 0.5703\n",
      "Epoch 14/1000\n",
      "56/56 [==============================] - 52s 922ms/step - loss: 1.4484 - acc: 0.5484 - val_loss: 1.3376 - val_acc: 0.6016\n",
      "Epoch 15/1000\n",
      "56/56 [==============================] - 51s 904ms/step - loss: 1.4016 - acc: 0.5621 - val_loss: 1.2864 - val_acc: 0.6172\n",
      "Epoch 16/1000\n",
      "56/56 [==============================] - 51s 914ms/step - loss: 1.3609 - acc: 0.5671 - val_loss: 1.2792 - val_acc: 0.6016\n",
      "Epoch 17/1000\n",
      "56/56 [==============================] - 52s 937ms/step - loss: 1.3326 - acc: 0.5769 - val_loss: 1.2842 - val_acc: 0.6250\n",
      "Epoch 18/1000\n",
      "56/56 [==============================] - 54s 963ms/step - loss: 1.2576 - acc: 0.5974 - val_loss: 1.1952 - val_acc: 0.6406\n",
      "Epoch 19/1000\n",
      "56/56 [==============================] - 50s 892ms/step - loss: 1.2789 - acc: 0.5815 - val_loss: 1.2187 - val_acc: 0.6250\n",
      "Epoch 20/1000\n",
      "56/56 [==============================] - 52s 921ms/step - loss: 1.2219 - acc: 0.5993 - val_loss: 1.2896 - val_acc: 0.6016\n",
      "Epoch 21/1000\n",
      "56/56 [==============================] - 50s 902ms/step - loss: 1.1882 - acc: 0.6203 - val_loss: 1.1826 - val_acc: 0.6406\n",
      "Epoch 22/1000\n",
      "56/56 [==============================] - 51s 914ms/step - loss: 1.1780 - acc: 0.6266 - val_loss: 1.2448 - val_acc: 0.6484\n",
      "Epoch 23/1000\n",
      "56/56 [==============================] - 52s 927ms/step - loss: 1.1955 - acc: 0.6213 - val_loss: 1.2438 - val_acc: 0.6250\n",
      "Epoch 24/1000\n",
      "56/56 [==============================] - 52s 922ms/step - loss: 1.0912 - acc: 0.6419 - val_loss: 1.1907 - val_acc: 0.6562\n",
      "Epoch 25/1000\n",
      "56/56 [==============================] - 52s 925ms/step - loss: 1.1076 - acc: 0.6313 - val_loss: 1.2027 - val_acc: 0.6484\n",
      "Epoch 26/1000\n",
      "56/56 [==============================] - 49s 878ms/step - loss: 1.0811 - acc: 0.6465 - val_loss: 1.2052 - val_acc: 0.6719\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (128,128)\n",
    "INPUT_SHAPE = (128,128,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(7,7), \n",
    "                                     strides=(2,2),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu')) \n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "              \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Dropout(0.25)) \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(32, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.25))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "112/112 [==============================] - 127s 1s/step - loss: 3.0368 - acc: 0.0882 - val_loss: 2.6109 - val_acc: 0.2313\n",
      "Epoch 2/1000\n",
      "112/112 [==============================] - 127s 1s/step - loss: 2.5311 - acc: 0.2525 - val_loss: 2.0425 - val_acc: 0.4125\n",
      "Epoch 3/1000\n",
      "112/112 [==============================] - 125s 1s/step - loss: 2.1095 - acc: 0.3657 - val_loss: 1.7335 - val_acc: 0.4813\n",
      "Epoch 4/1000\n",
      "112/112 [==============================] - 125s 1s/step - loss: 1.8208 - acc: 0.4592 - val_loss: 1.5732 - val_acc: 0.5188\n",
      "Epoch 5/1000\n",
      "112/112 [==============================] - 121s 1s/step - loss: 1.5987 - acc: 0.5054 - val_loss: 1.5318 - val_acc: 0.5687\n",
      "Epoch 6/1000\n",
      "112/112 [==============================] - 121s 1s/step - loss: 1.4283 - acc: 0.5574 - val_loss: 1.4741 - val_acc: 0.5625\n",
      "Epoch 7/1000\n",
      "112/112 [==============================] - 121s 1s/step - loss: 1.2236 - acc: 0.6106 - val_loss: 1.4337 - val_acc: 0.5875\n",
      "Epoch 8/1000\n",
      "112/112 [==============================] - 121s 1s/step - loss: 1.0632 - acc: 0.6643 - val_loss: 1.3292 - val_acc: 0.6375\n",
      "Epoch 9/1000\n",
      "112/112 [==============================] - 122s 1s/step - loss: 0.9563 - acc: 0.6822 - val_loss: 1.3495 - val_acc: 0.6375\n",
      "Epoch 10/1000\n",
      "112/112 [==============================] - 121s 1s/step - loss: 0.8527 - acc: 0.7260 - val_loss: 1.4013 - val_acc: 0.6438\n",
      "Epoch 11/1000\n",
      "112/112 [==============================] - 118s 1s/step - loss: 0.7655 - acc: 0.7468 - val_loss: 1.4345 - val_acc: 0.6250\n",
      "Epoch 12/1000\n",
      "112/112 [==============================] - 117s 1s/step - loss: 0.6887 - acc: 0.7688 - val_loss: 1.5922 - val_acc: 0.6312\n",
      "Epoch 13/1000\n",
      "112/112 [==============================] - 123s 1s/step - loss: 0.6584 - acc: 0.7752 - val_loss: 1.4898 - val_acc: 0.6188\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (32,32)\n",
    "INPUT_SHAPE = (32,32,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3), \n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))           \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(64, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(64, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(512, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.3))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "112/112 [==============================] - 10s 91ms/step - loss: 2.4908 - acc: 0.2506 - val_loss: 2.0344 - val_acc: 0.3563\n",
      "Epoch 2/1000\n",
      "112/112 [==============================] - 9s 78ms/step - loss: 1.9799 - acc: 0.3890 - val_loss: 1.8709 - val_acc: 0.4437\n",
      "Epoch 3/1000\n",
      "112/112 [==============================] - 8s 72ms/step - loss: 1.7059 - acc: 0.4682 - val_loss: 1.8202 - val_acc: 0.4562\n",
      "Epoch 4/1000\n",
      "112/112 [==============================] - 8s 71ms/step - loss: 1.4090 - acc: 0.5593 - val_loss: 2.0390 - val_acc: 0.4500\n",
      "Epoch 5/1000\n",
      "112/112 [==============================] - 8s 72ms/step - loss: 1.1566 - acc: 0.6319 - val_loss: 1.8169 - val_acc: 0.5000\n",
      "Epoch 6/1000\n",
      "112/112 [==============================] - 8s 72ms/step - loss: 0.8283 - acc: 0.7335 - val_loss: 1.9625 - val_acc: 0.4750\n",
      "Epoch 7/1000\n",
      "112/112 [==============================] - 8s 73ms/step - loss: 0.5617 - acc: 0.8252 - val_loss: 2.4491 - val_acc: 0.4938\n",
      "Epoch 8/1000\n",
      "112/112 [==============================] - 8s 73ms/step - loss: 0.3436 - acc: 0.8878 - val_loss: 2.7456 - val_acc: 0.5125\n",
      "Epoch 9/1000\n",
      "112/112 [==============================] - 8s 73ms/step - loss: 0.2328 - acc: 0.9281 - val_loss: 2.8933 - val_acc: 0.4625\n",
      "Epoch 10/1000\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.1674 - acc: 0.9461 - val_loss: 3.0322 - val_acc: 0.4938\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (128,128)\n",
    "INPUT_SHAPE = (128,128,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(7,7), \n",
    "                                     strides=(2,2),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))           \n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))        \n",
    "    model.add(tf.keras.layers.Dropout(0.3)) \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(64, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.3))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        #optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        optimizer=tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "112/112 [==============================] - 64s 570ms/step - loss: 2.8324 - acc: 0.1256 - val_loss: 2.3976 - val_acc: 0.3312\n",
      "Epoch 2/1000\n",
      "112/112 [==============================] - 63s 560ms/step - loss: 2.3354 - acc: 0.2937 - val_loss: 1.9252 - val_acc: 0.4875\n",
      "Epoch 3/1000\n",
      "112/112 [==============================] - 59s 523ms/step - loss: 1.9952 - acc: 0.3981 - val_loss: 1.7035 - val_acc: 0.5500\n",
      "Epoch 4/1000\n",
      "112/112 [==============================] - 62s 555ms/step - loss: 1.7516 - acc: 0.4651 - val_loss: 1.5771 - val_acc: 0.5563\n",
      "Epoch 5/1000\n",
      "112/112 [==============================] - 60s 536ms/step - loss: 1.6271 - acc: 0.5090 - val_loss: 1.4593 - val_acc: 0.5813\n",
      "Epoch 6/1000\n",
      "112/112 [==============================] - 59s 531ms/step - loss: 1.4718 - acc: 0.5449 - val_loss: 1.4329 - val_acc: 0.5563\n",
      "Epoch 7/1000\n",
      "112/112 [==============================] - 58s 521ms/step - loss: 1.3701 - acc: 0.5722 - val_loss: 1.3455 - val_acc: 0.6188\n",
      "Epoch 8/1000\n",
      "112/112 [==============================] - 59s 527ms/step - loss: 1.2741 - acc: 0.5936 - val_loss: 1.2706 - val_acc: 0.6500\n",
      "Epoch 9/1000\n",
      "112/112 [==============================] - 60s 538ms/step - loss: 1.1910 - acc: 0.6242 - val_loss: 1.2225 - val_acc: 0.6500\n",
      "Epoch 10/1000\n",
      "112/112 [==============================] - 60s 532ms/step - loss: 1.1854 - acc: 0.6162 - val_loss: 1.2796 - val_acc: 0.6438\n",
      "Epoch 11/1000\n",
      "112/112 [==============================] - 61s 542ms/step - loss: 1.0856 - acc: 0.6536 - val_loss: 1.2673 - val_acc: 0.6250\n",
      "Epoch 12/1000\n",
      "112/112 [==============================] - 60s 535ms/step - loss: 1.0289 - acc: 0.6628 - val_loss: 1.2457 - val_acc: 0.6875\n",
      "Epoch 13/1000\n",
      "112/112 [==============================] - 59s 528ms/step - loss: 0.9827 - acc: 0.6790 - val_loss: 1.2130 - val_acc: 0.6687\n",
      "Epoch 14/1000\n",
      "112/112 [==============================] - 59s 527ms/step - loss: 0.9587 - acc: 0.6879 - val_loss: 1.2806 - val_acc: 0.6312\n",
      "Epoch 15/1000\n",
      "112/112 [==============================] - 56s 497ms/step - loss: 0.8948 - acc: 0.6954 - val_loss: 1.2165 - val_acc: 0.6687\n",
      "Epoch 16/1000\n",
      "112/112 [==============================] - 57s 510ms/step - loss: 0.8646 - acc: 0.7098 - val_loss: 1.2580 - val_acc: 0.6312\n",
      "Epoch 17/1000\n",
      "112/112 [==============================] - 58s 518ms/step - loss: 0.8037 - acc: 0.7301 - val_loss: 1.2696 - val_acc: 0.6750\n",
      "Epoch 18/1000\n",
      "112/112 [==============================] - 55s 490ms/step - loss: 0.7622 - acc: 0.7479 - val_loss: 1.3303 - val_acc: 0.6750\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (128,128)\n",
    "INPUT_SHAPE = (128,128,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(7,7), \n",
    "                                     strides=(2,2),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))           \n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(3,3))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))        \n",
    "    model.add(tf.keras.layers.Dropout(0.3)) \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(64, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.3))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        #optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        optimizer=tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "112/112 [==============================] - 55s 493ms/step - loss: 2.9582 - acc: 0.0952 - val_loss: 2.5278 - val_acc: 0.3375\n",
      "Epoch 2/1000\n",
      "112/112 [==============================] - 53s 476ms/step - loss: 2.4523 - acc: 0.2596 - val_loss: 2.1166 - val_acc: 0.4250\n",
      "Epoch 3/1000\n",
      "112/112 [==============================] - 52s 461ms/step - loss: 2.1417 - acc: 0.3495 - val_loss: 1.7662 - val_acc: 0.5250\n",
      "Epoch 4/1000\n",
      "112/112 [==============================] - 51s 453ms/step - loss: 1.9281 - acc: 0.4147 - val_loss: 1.7295 - val_acc: 0.5250\n",
      "Epoch 5/1000\n",
      "112/112 [==============================] - 50s 447ms/step - loss: 1.7857 - acc: 0.4590 - val_loss: 1.5771 - val_acc: 0.5750\n",
      "Epoch 6/1000\n",
      "112/112 [==============================] - 49s 441ms/step - loss: 1.6522 - acc: 0.4885 - val_loss: 1.4748 - val_acc: 0.6000\n",
      "Epoch 7/1000\n",
      "112/112 [==============================] - 49s 441ms/step - loss: 1.5602 - acc: 0.5131 - val_loss: 1.4746 - val_acc: 0.5813\n",
      "Epoch 8/1000\n",
      "112/112 [==============================] - 43311s 387s/step - loss: 1.4699 - acc: 0.5337 - val_loss: 1.4399 - val_acc: 0.6062\n",
      "Epoch 9/1000\n",
      "112/112 [==============================] - 30332s 271s/step - loss: 1.3903 - acc: 0.5642 - val_loss: 1.3720 - val_acc: 0.6625\n",
      "Epoch 10/1000\n",
      "112/112 [==============================] - 46s 407ms/step - loss: 1.3089 - acc: 0.5802 - val_loss: 1.3051 - val_acc: 0.5875\n",
      "Epoch 11/1000\n",
      "112/112 [==============================] - 44s 395ms/step - loss: 1.2862 - acc: 0.5920 - val_loss: 1.2835 - val_acc: 0.6188\n",
      "Epoch 12/1000\n",
      "112/112 [==============================] - 42s 377ms/step - loss: 1.1968 - acc: 0.6071 - val_loss: 1.2240 - val_acc: 0.6500\n",
      "Epoch 13/1000\n",
      "112/112 [==============================] - 43s 385ms/step - loss: 1.1763 - acc: 0.6195 - val_loss: 1.2795 - val_acc: 0.6625\n",
      "Epoch 14/1000\n",
      "112/112 [==============================] - 44s 390ms/step - loss: 1.1125 - acc: 0.6356 - val_loss: 1.2908 - val_acc: 0.6375\n",
      "Epoch 15/1000\n",
      "112/112 [==============================] - 45s 403ms/step - loss: 1.1352 - acc: 0.6383 - val_loss: 1.2011 - val_acc: 0.6438\n",
      "Epoch 16/1000\n",
      "112/112 [==============================] - 43s 382ms/step - loss: 1.0280 - acc: 0.6713 - val_loss: 1.2068 - val_acc: 0.6438\n",
      "Epoch 17/1000\n",
      "112/112 [==============================] - 44s 392ms/step - loss: 1.0014 - acc: 0.6742 - val_loss: 1.2582 - val_acc: 0.6375\n",
      "Epoch 18/1000\n",
      "112/112 [==============================] - 47s 418ms/step - loss: 0.9851 - acc: 0.6753 - val_loss: 1.2827 - val_acc: 0.6500\n",
      "Epoch 19/1000\n",
      "112/112 [==============================] - 47s 420ms/step - loss: 0.9019 - acc: 0.7052 - val_loss: 1.2128 - val_acc: 0.6375\n",
      "Epoch 20/1000\n",
      "112/112 [==============================] - 48s 431ms/step - loss: 0.9048 - acc: 0.7073 - val_loss: 1.2389 - val_acc: 0.6438\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (128,128)\n",
    "INPUT_SHAPE = (128,128,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(7,7), \n",
    "                                     strides=(2,2),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))           \n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(3,3))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))        \n",
    "    model.add(tf.keras.layers.Dropout(0.3)) \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(64, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.3))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        #optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        optimizer=tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "54/54 [==============================] - 42s 782ms/step - loss: 2.9771 - acc: 0.0895 - val_loss: 2.5263 - val_acc: 0.2424\n",
      "Epoch 2/1000\n",
      "54/54 [==============================] - 41s 759ms/step - loss: 2.5481 - acc: 0.2166 - val_loss: 2.0953 - val_acc: 0.3939\n",
      "Epoch 3/1000\n",
      "54/54 [==============================] - 41s 760ms/step - loss: 2.2701 - acc: 0.3002 - val_loss: 1.8846 - val_acc: 0.4621\n",
      "Epoch 4/1000\n",
      "54/54 [==============================] - 41s 758ms/step - loss: 2.0833 - acc: 0.3640 - val_loss: 1.8568 - val_acc: 0.4545\n",
      "Epoch 5/1000\n",
      "54/54 [==============================] - 40s 735ms/step - loss: 1.9221 - acc: 0.4128 - val_loss: 1.6674 - val_acc: 0.5379\n",
      "Epoch 6/1000\n",
      "54/54 [==============================] - 41s 759ms/step - loss: 1.8353 - acc: 0.4408 - val_loss: 1.6373 - val_acc: 0.4924\n",
      "Epoch 7/1000\n",
      "54/54 [==============================] - 41s 755ms/step - loss: 1.7152 - acc: 0.4780 - val_loss: 1.5710 - val_acc: 0.5152\n",
      "Epoch 8/1000\n",
      "54/54 [==============================] - 41s 767ms/step - loss: 1.5917 - acc: 0.5045 - val_loss: 1.4628 - val_acc: 0.6061\n",
      "Epoch 9/1000\n",
      "54/54 [==============================] - 39s 718ms/step - loss: 1.5411 - acc: 0.5144 - val_loss: 1.4347 - val_acc: 0.5682\n",
      "Epoch 10/1000\n",
      "54/54 [==============================] - 41s 752ms/step - loss: 1.4585 - acc: 0.5397 - val_loss: 1.4085 - val_acc: 0.5682\n",
      "Epoch 11/1000\n",
      "54/54 [==============================] - 41s 758ms/step - loss: 1.3898 - acc: 0.5674 - val_loss: 1.3834 - val_acc: 0.5909\n",
      "Epoch 12/1000\n",
      "54/54 [==============================] - 42s 774ms/step - loss: 1.2988 - acc: 0.5833 - val_loss: 1.3657 - val_acc: 0.6212\n",
      "Epoch 13/1000\n",
      "54/54 [==============================] - 42s 776ms/step - loss: 1.2360 - acc: 0.6043 - val_loss: 1.3684 - val_acc: 0.6061\n",
      "Epoch 14/1000\n",
      "54/54 [==============================] - 40s 745ms/step - loss: 1.1875 - acc: 0.6226 - val_loss: 1.3355 - val_acc: 0.5909\n",
      "Epoch 15/1000\n",
      "54/54 [==============================] - 40s 738ms/step - loss: 1.1258 - acc: 0.6453 - val_loss: 1.3930 - val_acc: 0.6136\n",
      "Epoch 16/1000\n",
      "54/54 [==============================] - 41s 762ms/step - loss: 1.1063 - acc: 0.6497 - val_loss: 1.3484 - val_acc: 0.6136\n",
      "Epoch 17/1000\n",
      "54/54 [==============================] - 42s 771ms/step - loss: 1.0302 - acc: 0.6689 - val_loss: 1.3569 - val_acc: 0.5985\n",
      "Epoch 18/1000\n",
      "54/54 [==============================] - 40s 749ms/step - loss: 1.0134 - acc: 0.6742 - val_loss: 1.3595 - val_acc: 0.6136\n",
      "Epoch 19/1000\n",
      "54/54 [==============================] - 41s 764ms/step - loss: 0.9838 - acc: 0.6805 - val_loss: 1.4294 - val_acc: 0.6439\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (128,128)\n",
    "INPUT_SHAPE = (128,128,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(7,7), \n",
    "                                     strides=(2,2),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE, \n",
    "                                     name='l1'))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))           \n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None, \n",
    "                                     name='l2'))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None, \n",
    "                                     name='l3'))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None, \n",
    "                                     name='l4'))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))        \n",
    "    model.add(tf.keras.layers.Dropout(0.3)) \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(64, activation=None, name='l5'))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.3))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None, name='l6'))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        #optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        optimizer=tf.keras.optimizers.SGD(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "l1 (Conv2D)                  (None, 64, 64, 32)        1600      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "l2 (Conv2D)                  (None, 64, 64, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "l3 (Conv2D)                  (None, 32, 32, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "l4 (Conv2D)                  (None, 16, 16, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "l5 (Dense)                   (None, 64)                131136    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "l6 (Dense)                   (None, 22)                1430      \n",
      "_________________________________________________________________\n",
      "output (Activation)          (None, 22)                0         \n",
      "=================================================================\n",
      "Total params: 211,062\n",
      "Trainable params: 211,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3595 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "54/54 [==============================] - 46s 854ms/step - loss: 3.0929 - acc: 0.0424 - val_loss: 3.0906 - val_acc: 0.0455\n",
      "Epoch 2/1000\n",
      "54/54 [==============================] - 46s 851ms/step - loss: 3.0918 - acc: 0.0393 - val_loss: 3.0899 - val_acc: 0.0455\n",
      "Epoch 3/1000\n",
      "54/54 [==============================] - 43s 805ms/step - loss: 3.0904 - acc: 0.0447 - val_loss: 3.0889 - val_acc: 0.0758\n",
      "Epoch 4/1000\n",
      "54/54 [==============================] - 46s 846ms/step - loss: 3.0894 - acc: 0.0488 - val_loss: 3.0881 - val_acc: 0.1136\n",
      "Epoch 5/1000\n",
      "54/54 [==============================] - 44s 817ms/step - loss: 3.0893 - acc: 0.0591 - val_loss: 3.0873 - val_acc: 0.0985\n",
      "Epoch 6/1000\n",
      "54/54 [==============================] - 44s 813ms/step - loss: 3.0883 - acc: 0.0604 - val_loss: 3.0858 - val_acc: 0.1439\n",
      "Epoch 7/1000\n",
      "54/54 [==============================] - 44s 812ms/step - loss: 3.0870 - acc: 0.0604 - val_loss: 3.0845 - val_acc: 0.1591\n",
      "Epoch 8/1000\n",
      "54/54 [==============================] - 43s 804ms/step - loss: 3.0855 - acc: 0.0747 - val_loss: 3.0828 - val_acc: 0.1439\n",
      "Epoch 9/1000\n",
      "54/54 [==============================] - 43s 801ms/step - loss: 3.0849 - acc: 0.0663 - val_loss: 3.0809 - val_acc: 0.1515\n",
      "Epoch 10/1000\n",
      "54/54 [==============================] - 43s 801ms/step - loss: 3.0832 - acc: 0.0679 - val_loss: 3.0789 - val_acc: 0.1439\n",
      "Epoch 11/1000\n",
      "54/54 [==============================] - 44s 811ms/step - loss: 3.0793 - acc: 0.0694 - val_loss: 3.0744 - val_acc: 0.2045\n",
      "Epoch 12/1000\n",
      "54/54 [==============================] - 44s 810ms/step - loss: 3.0758 - acc: 0.0826 - val_loss: 3.0679 - val_acc: 0.1515\n",
      "Epoch 13/1000\n",
      "54/54 [==============================] - 44s 811ms/step - loss: 3.0721 - acc: 0.0736 - val_loss: 3.0595 - val_acc: 0.2273\n",
      "Epoch 14/1000\n",
      "54/54 [==============================] - 45s 827ms/step - loss: 3.0635 - acc: 0.0836 - val_loss: 3.0446 - val_acc: 0.2197\n",
      "Epoch 15/1000\n",
      "54/54 [==============================] - 43s 801ms/step - loss: 3.0468 - acc: 0.0832 - val_loss: 3.0158 - val_acc: 0.2348\n",
      "Epoch 16/1000\n",
      "54/54 [==============================] - 43s 802ms/step - loss: 3.0163 - acc: 0.0890 - val_loss: 2.9542 - val_acc: 0.2045\n",
      "Epoch 17/1000\n",
      "54/54 [==============================] - 43s 797ms/step - loss: 2.9770 - acc: 0.1006 - val_loss: 2.8670 - val_acc: 0.2197\n",
      "Epoch 18/1000\n",
      "54/54 [==============================] - 43s 796ms/step - loss: 2.9268 - acc: 0.1110 - val_loss: 2.7859 - val_acc: 0.1818\n",
      "Epoch 19/1000\n",
      "54/54 [==============================] - 43s 802ms/step - loss: 2.9050 - acc: 0.1202 - val_loss: 2.7394 - val_acc: 0.2424\n",
      "Epoch 20/1000\n",
      "54/54 [==============================] - 44s 806ms/step - loss: 2.8251 - acc: 0.1355 - val_loss: 2.6299 - val_acc: 0.2652\n",
      "Epoch 21/1000\n",
      "54/54 [==============================] - 43s 802ms/step - loss: 2.7955 - acc: 0.1534 - val_loss: 2.5830 - val_acc: 0.2500\n",
      "Epoch 22/1000\n",
      "54/54 [==============================] - 43s 789ms/step - loss: 2.7413 - acc: 0.1629 - val_loss: 2.5094 - val_acc: 0.2500\n",
      "Epoch 23/1000\n",
      "54/54 [==============================] - 43s 799ms/step - loss: 2.7091 - acc: 0.1799 - val_loss: 2.4517 - val_acc: 0.3333\n",
      "Epoch 24/1000\n",
      "54/54 [==============================] - 43s 791ms/step - loss: 2.6335 - acc: 0.2111 - val_loss: 2.4106 - val_acc: 0.3409\n",
      "Epoch 25/1000\n",
      "54/54 [==============================] - 43s 802ms/step - loss: 2.6204 - acc: 0.2086 - val_loss: 2.3639 - val_acc: 0.3636\n",
      "Epoch 26/1000\n",
      "54/54 [==============================] - 43s 805ms/step - loss: 2.5646 - acc: 0.2276 - val_loss: 2.3367 - val_acc: 0.3485\n",
      "Epoch 27/1000\n",
      "54/54 [==============================] - 43s 799ms/step - loss: 2.5600 - acc: 0.2170 - val_loss: 2.2849 - val_acc: 0.3636\n",
      "Epoch 28/1000\n",
      "54/54 [==============================] - 45s 837ms/step - loss: 2.4936 - acc: 0.2413 - val_loss: 2.1980 - val_acc: 0.3939\n",
      "Epoch 29/1000\n",
      "54/54 [==============================] - 47s 873ms/step - loss: 2.4788 - acc: 0.2548 - val_loss: 2.2025 - val_acc: 0.4091\n",
      "Epoch 30/1000\n",
      "54/54 [==============================] - 48s 880ms/step - loss: 2.4314 - acc: 0.2663 - val_loss: 2.1667 - val_acc: 0.4242\n",
      "Epoch 31/1000\n",
      "54/54 [==============================] - 47s 872ms/step - loss: 2.4323 - acc: 0.2697 - val_loss: 2.1456 - val_acc: 0.4318\n",
      "Epoch 32/1000\n",
      "54/54 [==============================] - 47s 870ms/step - loss: 2.3881 - acc: 0.2728 - val_loss: 2.1206 - val_acc: 0.4621\n",
      "Epoch 33/1000\n",
      "54/54 [==============================] - 47s 869ms/step - loss: 2.3857 - acc: 0.2867 - val_loss: 2.0895 - val_acc: 0.4621\n",
      "Epoch 34/1000\n",
      "54/54 [==============================] - 47s 863ms/step - loss: 2.3537 - acc: 0.2883 - val_loss: 2.0538 - val_acc: 0.4697\n",
      "Epoch 35/1000\n",
      "54/54 [==============================] - 45s 833ms/step - loss: 2.3263 - acc: 0.2966 - val_loss: 2.0362 - val_acc: 0.4773\n",
      "Epoch 36/1000\n",
      "54/54 [==============================] - 43s 794ms/step - loss: 2.2864 - acc: 0.3132 - val_loss: 2.0105 - val_acc: 0.4697\n",
      "Epoch 37/1000\n",
      "54/54 [==============================] - 43s 794ms/step - loss: 2.2566 - acc: 0.3172 - val_loss: 1.9630 - val_acc: 0.5152\n",
      "Epoch 38/1000\n",
      "54/54 [==============================] - 43s 799ms/step - loss: 2.2697 - acc: 0.3208 - val_loss: 1.9633 - val_acc: 0.4621\n",
      "Epoch 39/1000\n",
      "54/54 [==============================] - 43s 800ms/step - loss: 2.2533 - acc: 0.3279 - val_loss: 1.9538 - val_acc: 0.5076\n",
      "Epoch 40/1000\n",
      "54/54 [==============================] - 43s 792ms/step - loss: 2.1947 - acc: 0.3321 - val_loss: 1.9485 - val_acc: 0.4924\n",
      "Epoch 41/1000\n",
      "54/54 [==============================] - 44s 807ms/step - loss: 2.1632 - acc: 0.3555 - val_loss: 1.8869 - val_acc: 0.5152\n",
      "Epoch 42/1000\n",
      "54/54 [==============================] - 43s 794ms/step - loss: 2.1886 - acc: 0.3439 - val_loss: 1.9001 - val_acc: 0.4848\n",
      "Epoch 43/1000\n",
      "54/54 [==============================] - 44s 807ms/step - loss: 2.1471 - acc: 0.3370 - val_loss: 1.9131 - val_acc: 0.4470\n",
      "Epoch 44/1000\n",
      "54/54 [==============================] - 43s 799ms/step - loss: 2.0915 - acc: 0.3629 - val_loss: 1.8366 - val_acc: 0.4773\n",
      "Epoch 45/1000\n",
      "54/54 [==============================] - 43s 793ms/step - loss: 2.0945 - acc: 0.3678 - val_loss: 1.8399 - val_acc: 0.5152\n",
      "Epoch 46/1000\n",
      "54/54 [==============================] - 43s 794ms/step - loss: 2.0442 - acc: 0.3798 - val_loss: 1.7968 - val_acc: 0.4848\n",
      "Epoch 47/1000\n",
      "54/54 [==============================] - 43s 796ms/step - loss: 2.0564 - acc: 0.3705 - val_loss: 1.8221 - val_acc: 0.5000\n",
      "Epoch 48/1000\n",
      "54/54 [==============================] - 43s 794ms/step - loss: 2.0668 - acc: 0.3744 - val_loss: 1.8235 - val_acc: 0.4924\n",
      "Epoch 49/1000\n",
      "54/54 [==============================] - 43s 792ms/step - loss: 2.0527 - acc: 0.3804 - val_loss: 1.8113 - val_acc: 0.5000\n",
      "Epoch 50/1000\n",
      "54/54 [==============================] - 43s 788ms/step - loss: 2.0139 - acc: 0.3943 - val_loss: 1.8071 - val_acc: 0.4848\n",
      "Epoch 51/1000\n",
      "54/54 [==============================] - 43s 804ms/step - loss: 1.9740 - acc: 0.4080 - val_loss: 1.7624 - val_acc: 0.5000\n",
      "Epoch 52/1000\n",
      "54/54 [==============================] - 43s 793ms/step - loss: 1.9755 - acc: 0.4097 - val_loss: 1.7288 - val_acc: 0.5303\n",
      "Epoch 53/1000\n",
      "54/54 [==============================] - 44s 806ms/step - loss: 1.9366 - acc: 0.4093 - val_loss: 1.6990 - val_acc: 0.5152\n",
      "Epoch 54/1000\n",
      "54/54 [==============================] - 43s 801ms/step - loss: 1.9509 - acc: 0.4102 - val_loss: 1.7258 - val_acc: 0.5152\n",
      "Epoch 55/1000\n",
      "54/54 [==============================] - 43s 800ms/step - loss: 1.9027 - acc: 0.4249 - val_loss: 1.7020 - val_acc: 0.5152\n",
      "Epoch 56/1000\n",
      "54/54 [==============================] - 44s 807ms/step - loss: 1.8952 - acc: 0.4198 - val_loss: 1.6948 - val_acc: 0.5152\n",
      "Epoch 57/1000\n",
      "54/54 [==============================] - 43s 798ms/step - loss: 1.8719 - acc: 0.4305 - val_loss: 1.6611 - val_acc: 0.5303\n",
      "Epoch 58/1000\n",
      "54/54 [==============================] - 43s 799ms/step - loss: 1.8387 - acc: 0.4385 - val_loss: 1.6584 - val_acc: 0.5303\n",
      "Epoch 59/1000\n",
      "54/54 [==============================] - 43s 803ms/step - loss: 1.8856 - acc: 0.4270 - val_loss: 1.6692 - val_acc: 0.5455\n",
      "Epoch 60/1000\n",
      "54/54 [==============================] - 43s 800ms/step - loss: 1.8177 - acc: 0.4384 - val_loss: 1.6500 - val_acc: 0.5379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000\n",
      "54/54 [==============================] - 43s 801ms/step - loss: 1.8033 - acc: 0.4636 - val_loss: 1.6274 - val_acc: 0.5455\n",
      "Epoch 62/1000\n",
      "54/54 [==============================] - 43s 800ms/step - loss: 1.8030 - acc: 0.4418 - val_loss: 1.6404 - val_acc: 0.5076\n",
      "Epoch 63/1000\n",
      "54/54 [==============================] - 43s 805ms/step - loss: 1.7881 - acc: 0.4523 - val_loss: 1.6247 - val_acc: 0.5227\n",
      "Epoch 64/1000\n",
      "54/54 [==============================] - 43s 797ms/step - loss: 1.7699 - acc: 0.4541 - val_loss: 1.5861 - val_acc: 0.5455\n",
      "Epoch 65/1000\n",
      "54/54 [==============================] - 44s 808ms/step - loss: 1.7529 - acc: 0.4607 - val_loss: 1.5984 - val_acc: 0.5682\n",
      "Epoch 66/1000\n",
      "54/54 [==============================] - 43s 788ms/step - loss: 1.7135 - acc: 0.4766 - val_loss: 1.5729 - val_acc: 0.5682\n",
      "Epoch 67/1000\n",
      "54/54 [==============================] - 43s 799ms/step - loss: 1.7327 - acc: 0.4716 - val_loss: 1.5695 - val_acc: 0.5606\n",
      "Epoch 68/1000\n",
      "54/54 [==============================] - 43s 796ms/step - loss: 1.6980 - acc: 0.4840 - val_loss: 1.5635 - val_acc: 0.5530\n",
      "Epoch 69/1000\n",
      "54/54 [==============================] - 43s 797ms/step - loss: 1.7040 - acc: 0.4689 - val_loss: 1.5472 - val_acc: 0.5530\n",
      "Epoch 70/1000\n",
      "54/54 [==============================] - 43s 797ms/step - loss: 1.6836 - acc: 0.4769 - val_loss: 1.5835 - val_acc: 0.5379\n",
      "Epoch 71/1000\n",
      "54/54 [==============================] - 43s 800ms/step - loss: 1.6730 - acc: 0.4882 - val_loss: 1.5418 - val_acc: 0.5455\n",
      "Epoch 72/1000\n",
      "54/54 [==============================] - 43s 804ms/step - loss: 1.6530 - acc: 0.4966 - val_loss: 1.5179 - val_acc: 0.5833\n",
      "Epoch 73/1000\n",
      "54/54 [==============================] - 44s 806ms/step - loss: 1.6171 - acc: 0.5019 - val_loss: 1.5118 - val_acc: 0.5758\n",
      "Epoch 74/1000\n",
      "54/54 [==============================] - 43s 797ms/step - loss: 1.6447 - acc: 0.4894 - val_loss: 1.4917 - val_acc: 0.5758\n",
      "Epoch 75/1000\n",
      "54/54 [==============================] - 44s 806ms/step - loss: 1.6242 - acc: 0.4989 - val_loss: 1.5027 - val_acc: 0.5379\n",
      "Epoch 76/1000\n",
      "54/54 [==============================] - 43s 797ms/step - loss: 1.5842 - acc: 0.5134 - val_loss: 1.5091 - val_acc: 0.5606\n",
      "Epoch 77/1000\n",
      "54/54 [==============================] - 43s 802ms/step - loss: 1.5962 - acc: 0.5069 - val_loss: 1.4975 - val_acc: 0.5909\n",
      "Epoch 78/1000\n",
      "54/54 [==============================] - 43s 802ms/step - loss: 1.5668 - acc: 0.4999 - val_loss: 1.5254 - val_acc: 0.5606\n",
      "Epoch 79/1000\n",
      "54/54 [==============================] - 43s 802ms/step - loss: 1.5650 - acc: 0.5180 - val_loss: 1.4835 - val_acc: 0.5833\n",
      "Epoch 80/1000\n",
      "54/54 [==============================] - 44s 811ms/step - loss: 1.5278 - acc: 0.5157 - val_loss: 1.4974 - val_acc: 0.5303\n",
      "Epoch 81/1000\n",
      "54/54 [==============================] - 43s 795ms/step - loss: 1.5324 - acc: 0.5141 - val_loss: 1.4822 - val_acc: 0.5682\n",
      "Epoch 82/1000\n",
      "54/54 [==============================] - 43s 802ms/step - loss: 1.5064 - acc: 0.5335 - val_loss: 1.4582 - val_acc: 0.5606\n",
      "Epoch 83/1000\n",
      "54/54 [==============================] - 43s 800ms/step - loss: 1.5098 - acc: 0.5250 - val_loss: 1.4887 - val_acc: 0.5758\n",
      "Epoch 84/1000\n",
      "54/54 [==============================] - 43s 803ms/step - loss: 1.5184 - acc: 0.5209 - val_loss: 1.4829 - val_acc: 0.5379\n",
      "Epoch 85/1000\n",
      "54/54 [==============================] - 44s 810ms/step - loss: 1.5163 - acc: 0.5230 - val_loss: 1.4682 - val_acc: 0.5909\n",
      "Epoch 86/1000\n",
      "54/54 [==============================] - 43s 801ms/step - loss: 1.4503 - acc: 0.5475 - val_loss: 1.4411 - val_acc: 0.5682\n",
      "Epoch 87/1000\n",
      "54/54 [==============================] - 43s 803ms/step - loss: 1.4802 - acc: 0.5329 - val_loss: 1.4760 - val_acc: 0.5606\n",
      "Epoch 88/1000\n",
      "54/54 [==============================] - 44s 814ms/step - loss: 1.4640 - acc: 0.5474 - val_loss: 1.4702 - val_acc: 0.5682\n",
      "Epoch 89/1000\n",
      "54/54 [==============================] - 43s 801ms/step - loss: 1.4380 - acc: 0.5467 - val_loss: 1.4227 - val_acc: 0.6061\n",
      "Epoch 90/1000\n",
      "54/54 [==============================] - 44s 809ms/step - loss: 1.4566 - acc: 0.5385 - val_loss: 1.4309 - val_acc: 0.5909\n",
      "Epoch 91/1000\n",
      "54/54 [==============================] - 44s 811ms/step - loss: 1.4053 - acc: 0.5511 - val_loss: 1.4841 - val_acc: 0.5833\n",
      "Epoch 92/1000\n",
      "54/54 [==============================] - 44s 808ms/step - loss: 1.3983 - acc: 0.5455 - val_loss: 1.4281 - val_acc: 0.5909\n",
      "Epoch 93/1000\n",
      "54/54 [==============================] - 43s 799ms/step - loss: 1.4085 - acc: 0.5526 - val_loss: 1.4325 - val_acc: 0.5985\n",
      "Epoch 94/1000\n",
      "54/54 [==============================] - 44s 809ms/step - loss: 1.3755 - acc: 0.5626 - val_loss: 1.4161 - val_acc: 0.6212\n",
      "Epoch 95/1000\n",
      "54/54 [==============================] - 44s 808ms/step - loss: 1.3792 - acc: 0.5646 - val_loss: 1.4156 - val_acc: 0.5909\n",
      "Epoch 96/1000\n",
      "54/54 [==============================] - 43s 803ms/step - loss: 1.3517 - acc: 0.5814 - val_loss: 1.5362 - val_acc: 0.5909\n",
      "Epoch 97/1000\n",
      "54/54 [==============================] - 44s 809ms/step - loss: 1.3683 - acc: 0.5654 - val_loss: 1.4431 - val_acc: 0.5985\n",
      "Epoch 98/1000\n",
      "54/54 [==============================] - 44s 809ms/step - loss: 1.3662 - acc: 0.5659 - val_loss: 1.4569 - val_acc: 0.5833\n",
      "Epoch 99/1000\n",
      "54/54 [==============================] - 43s 805ms/step - loss: 1.3294 - acc: 0.5792 - val_loss: 1.4403 - val_acc: 0.6061\n",
      "Epoch 100/1000\n",
      "54/54 [==============================] - 44s 810ms/step - loss: 1.3172 - acc: 0.5745 - val_loss: 1.4387 - val_acc: 0.5833\n"
     ]
    }
   ],
   "source": [
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 176 images belonging to 22 classes.\n",
      "('Test loss:', 1.4034723043441772)\n",
      "('Test accuracy:', 0.6079545468091965)\n"
     ]
    }
   ],
   "source": [
    "score = validate_model(model)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of each entry in array is given by -dataType. The number of entries is equal to:\n",
    "\n",
    "inputFeatureChannels outputFeatureChannels kernelHeight kernelWidth*\n",
    "\n",
    "The layout of filter weight is as a 4D tensor (array) weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ inputChannels / groups ]\n",
    "\n",
    "Note: For binary-convolutions the layout of the weights are: weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ floor((inputChannels/groups)+31) / 32 ] with each 32 sub input feature channel index specified in machine byte order, so that for example the 13th feature channel bit can be extracted using bitmask = (1U << 13)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_conv_weights(name, wts_coef, bias_coef):\n",
    "    print(\"Exporting weights for {}\\n\\t{}\\n\\t{}\".format(name, \n",
    "          os.path.join('exports', \"{}_conv.data\".format(name)), \n",
    "          os.path.join('exports', \"{}_bias.data\".format(name))))\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # [kernel_width, kernel_height, input_feature_channels, output_feature_channels]\n",
    "    print(\"\\tOriginal weights shape {}\".format(wts_coef.shape))\n",
    "    if bias_coef is not None:\n",
    "        # [output_feature_channels]\n",
    "        print(\"\\tOriginal bias shape {}\".format(bias_coef.shape))\n",
    "    \n",
    "    # [output_feature_channels, kernel_width, kernel_height, input_feature_channels]\n",
    "    wts_coef = wts_coef.transpose(3, 0, 1, 2)\n",
    "    print(\"\\tReshaped weights shape {}\".format(wts_coef.shape))    \n",
    "    wts_coef.tofile(os.path.join('exports', \"{}_conv.data\".format(name)))    \n",
    "    \n",
    "    if bias_coef is not None:\n",
    "        bias_coef = np.squeeze(bias_coef)\n",
    "        print(\"\\tReshaped bias_coef shape {}\".format(bias_coef.shape))    \n",
    "        bias_coef.tofile(os.path.join('exports', \"{}_bias.data\".format(name)))    \n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dense_weights(name, wts_coef, bias_coef, kernel_width, kernel_height):\n",
    "    \"\"\"\n",
    "    A fully connected layer in a Convolutional Neural Network (CNN) is one where every input channel is connected \n",
    "    to every output channel. The kernel width is equal to the width of the source image, and the \n",
    "    kernel height is equal to the height of the source image. The width and height of the output is 1 x 1.\n",
    "    \n",
    "    A fully connected layer takes an MPSImage object with dimensions \n",
    "    source.width x source.height x Ni, convolves it with Weights[No][source.width][source.height][Ni], \n",
    "    and produces a 1 x 1 x No output.\n",
    "    \n",
    "    Thus, the following conditions must be true:\n",
    "    - kernelWidth == source.width\n",
    "    - kernelHeight == source.height\n",
    "    - clipRect.size.width == 1\n",
    "    - clipRect.size.height == 1\n",
    "    \n",
    "    You can think of a fully connected layer as a matrix multiplication where the image is \n",
    "    flattened into a vector of length source.width*source.height*Ni, and the weights are arranged in a \n",
    "    matrix of dimension No x (source.width*source.height*Ni) to produce an output vector of length No.\n",
    "    \n",
    "    The value of the strideInPixelsX, strideInPixelsY, and groups properties must be 1. \n",
    "    The offset property is not applicable and it is ignored. Because the clip rectangle is \n",
    "    clamped to the destination image bounds, if the destination is 1 x 1, you do not need to set the \n",
    "    clipRect property.\n",
    "    \"\"\"\n",
    "    print(\"Exporting weights for {}\\n\\t{}\\n\\t{}\".format(name, \n",
    "          os.path.join('exports', \"{}_conv.data\".format(name)), \n",
    "          os.path.join('exports', \"{}_bias.data\".format(name))))\n",
    "        \n",
    "    input_feature_channels = int(wts_coef.shape[0] / kernel_width / kernel_height) \n",
    "    output_feature_channels = wts_coef.shape[-1]            \n",
    "    \n",
    "    # [kernel_width, kernel_height, input_feature_channels, output_feature_channels]\n",
    "    print(\"\\tOriginal weights shape {}\".format(wts_coef.shape))\n",
    "    \n",
    "    #wts_coef = np.reshape(wts_coef, [kernel_width, kernel_height, input_feature_channels, output_feature_channels])    \n",
    "    wts_coef = np.reshape(wts_coef, [kernel_width, kernel_height, -1, output_feature_channels])    \n",
    "        \n",
    "    if bias_coef is not None:\n",
    "        # [output_feature_channels]\n",
    "        print(\"\\tOriginal bias shape {}\".format(bias_coef.shape))\n",
    "    \n",
    "    # [output_feature_channels, kernel_width, kernel_height, input_feature_channels]\n",
    "    wts_coef = wts_coef.transpose(3, 0, 1, 2)\n",
    "    print(\"\\tReshaped weights shape {}\".format(wts_coef.shape))    \n",
    "    wts_coef.tofile(os.path.join('exports', \"{}_conv.data\".format(name)))    \n",
    "    \n",
    "    if bias_coef is not None:\n",
    "        bias_coef = np.squeeze(bias_coef)\n",
    "        print(\"\\tReshaped bias_coef shape {}\".format(bias_coef.shape))    \n",
    "        bias_coef.tofile(os.path.join('exports', \"{}_bias.data\".format(name)))   \n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting weights for l1\n",
      "\texports/l1_conv.data\n",
      "\texports/l1_bias.data\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-3e31bac36661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             export_dense_weights(layer.name, wts[0], wts[1] if len(wts) == 2 else None, \n\u001b[0;32m---> 17\u001b[0;31m                                 flatted_input_kernel_width, flatted_input_kernel_height)        \n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;31m# after the initial pass (from cnn to fcn); flattern the kernel down to 1x1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# i.e. update the flatted_input_kernel_DIM to have the kernel width and height of 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-1a7a64bbc34e>\u001b[0m in \u001b[0;36mexport_dense_weights\u001b[0;34m(name, wts_coef, bias_coef, kernel_width, kernel_height)\u001b[0m\n\u001b[1;32m     28\u001b[0m           os.path.join('exports', \"{}_bias.data\".format(name))))\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0minput_feature_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwts_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkernel_width\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkernel_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0moutput_feature_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwts_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "flatted_input_kernel_width = None\n",
    "flatted_input_kernel_height = None\n",
    "\n",
    "for layer in model.layers:        \n",
    "    if \"flatten\" in layer.name:\n",
    "        flatted_input_kernel_width = layer.input_shape[1] \n",
    "        flatted_input_kernel_height = layer.input_shape[2] \n",
    "        \n",
    "    if len(layer.get_weights()) > 0:        \n",
    "        name = layer.name         \n",
    "        wts = layer.get_weights()\n",
    "        \n",
    "        if \"conv\" in name:\n",
    "            export_conv_weights(layer.name, wts[0], wts[1] if len(wts) == 2 else None)        \n",
    "        else:\n",
    "            export_dense_weights(layer.name, wts[0], wts[1] if len(wts) == 2 else None, \n",
    "                                flatted_input_kernel_width, flatted_input_kernel_height)        \n",
    "            # after the initial pass (from cnn to fcn); flattern the kernel down to 1x1 \n",
    "            # i.e. update the flatted_input_kernel_DIM to have the kernel width and height of 1 \n",
    "            flatted_input_kernel_width, flatted_input_kernel_height = 1, 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4swift27",
   "language": "python",
   "name": "dl4swift27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
