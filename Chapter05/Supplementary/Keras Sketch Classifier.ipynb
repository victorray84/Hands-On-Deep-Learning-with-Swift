{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/Users/joshua.newnham/Documents/Shared Playground Data/Sketches/preprocessed/'\n",
    "VALID_DIR = os.path.join(ROOT_DIR, \"valid\")\n",
    "TRAIN_DIR = os.path.join(ROOT_DIR, \"train\")\n",
    "WEIGHTS_FILE = \"sketch_classifier.h5\"\n",
    "\n",
    "TARGET_SIZE = (128,128)\n",
    "INPUT_SHAPE = (128,128,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files(full_path):\n",
    "    count = 0 \n",
    "    def is_image(file_path):\n",
    "        image_extensions = ['png', 'jpg', 'jpeg']\n",
    "        \n",
    "        for image_extension in image_extensions:\n",
    "            if image_extension in file_path.lower():\n",
    "                return True\n",
    "            \n",
    "        return False \n",
    "    \n",
    "    for d in os.listdir(full_path):\n",
    "        if not os.path.isdir(os.path.join(full_path, d)):\n",
    "            continue\n",
    "            \n",
    "        sub_full_path = os.path.join(full_path, d)\n",
    "        \n",
    "        for f in os.listdir(sub_full_path):\n",
    "            img_path = os.path.join(sub_full_path, f)\n",
    "            if os.path.isfile(img_path) and is_image(img_path):\n",
    "                count += 1\n",
    "            \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_FILES = count_files(TRAIN_DIR)\n",
    "NUM_VALID_FILES = count_files(VALID_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    \n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.)\n",
    "    \n",
    "    # create an iterator for the training data \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        color_mode='grayscale')\n",
    "    \n",
    "    # create an iterator for the validation data \n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        VALID_DIR,\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        color_mode='grayscale')\n",
    "    \n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(WEIGHTS_FILE, \n",
    "                                           monitor='val_loss', \n",
    "                                           verbose=0, \n",
    "                                           save_best_only=True, \n",
    "                                           save_weights_only=True, \n",
    "                                           mode='auto', \n",
    "                                           period=2)\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=int(NUM_TRAIN_FILES/BATCH_SIZE),\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=int(NUM_VALID_FILES/BATCH_SIZE), \n",
    "        callbacks=[checkpoint, early_stopping]) \n",
    "    \n",
    "    return history, model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model):\n",
    "    \n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.)\n",
    "    \n",
    "    # create an iterator for the validation data \n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        VALID_DIR,\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        color_mode='grayscale')\n",
    "    \n",
    "    score = model.evaluate_generator(validation_generator)\n",
    "    \n",
    "    return score "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    65% accuracy input size 68\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3), \n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE, \n",
    "                                     name=\"l1_conv\"))    \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l1_relu\"))    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3), \n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     name=\"l1_conv_b\"))    \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l1_relu_b\"))    \n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, name=\"l1_maxpool\", padding='same'))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None, \n",
    "                                     name=\"l2_conv\"))            \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l2_relu\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, name=\"l2_maxpool\", padding='same')) \n",
    "    model.add(tf.keras.layers.Dropout(0.35, name=\"l3_dropout\"))\n",
    "        \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(129, \n",
    "                                    activation=None, \n",
    "                                    name='l4_fc'))    \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l4_relu\"))\n",
    "    model.add(tf.keras.layers.Dropout(0.35, name=\"l4_dropout\"))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, \n",
    "                                    activation=None, \n",
    "                                    name='l5_fc'))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "history, model = train(create_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    65% accuracy input size 68\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3), \n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE, \n",
    "                                     name=\"l1_conv\"))    \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l1_relu\"))    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3), \n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     name=\"l1_conv_b\"))    \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l1_relu_b\"))    \n",
    "    model.add(tf.keras.layers.MaxPooling2D(3,3, name=\"l1_maxpool\", padding='same'))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None, \n",
    "                                     name=\"l2_conv\"))            \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l2_relu\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, name=\"l2_maxpool\", padding='same')) \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None, \n",
    "                                     name=\"l3_conv\"))        \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l3_relu\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, name=\"l3_maxpool\", padding='same')) \n",
    "    model.add(tf.keras.layers.Dropout(0.4, name=\"l3_dropout\"))\n",
    "        \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(512, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(256, \n",
    "                                    activation=None, \n",
    "                                    name='l4_fc'))    \n",
    "    model.add(tf.keras.layers.Activation('relu', name=\"l4_relu\"))\n",
    "    model.add(tf.keras.layers.Dropout(0.4, name=\"l4_dropout\"))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, \n",
    "                                    activation=None, \n",
    "                                    name='l5_fc'))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "450/450 [==============================] - 25s 57ms/step - loss: 2.5642 - acc: 0.2110 - val_loss: 1.9611 - val_acc: 0.3864\n",
      "Epoch 2/1000\n",
      "450/450 [==============================] - 24s 54ms/step - loss: 2.0212 - acc: 0.3814 - val_loss: 1.8084 - val_acc: 0.4943\n",
      "Epoch 3/1000\n",
      "450/450 [==============================] - 23s 51ms/step - loss: 1.7381 - acc: 0.4638 - val_loss: 1.6036 - val_acc: 0.5057\n",
      "Epoch 4/1000\n",
      "450/450 [==============================] - 22s 48ms/step - loss: 1.5279 - acc: 0.5251 - val_loss: 1.5731 - val_acc: 0.5341\n",
      "Epoch 5/1000\n",
      "450/450 [==============================] - 25s 55ms/step - loss: 1.3486 - acc: 0.5730 - val_loss: 1.4252 - val_acc: 0.6023\n",
      "Epoch 6/1000\n",
      "450/450 [==============================] - 20s 45ms/step - loss: 1.2041 - acc: 0.6149 - val_loss: 1.4535 - val_acc: 0.6250\n",
      "Epoch 7/1000\n",
      "450/450 [==============================] - 26s 57ms/step - loss: 1.0633 - acc: 0.6630 - val_loss: 1.3939 - val_acc: 0.6420\n",
      "Epoch 8/1000\n",
      "450/450 [==============================] - 21s 47ms/step - loss: 0.9626 - acc: 0.6897 - val_loss: 1.4401 - val_acc: 0.6136\n",
      "Epoch 9/1000\n",
      "450/450 [==============================] - 22s 49ms/step - loss: 0.8740 - acc: 0.7181 - val_loss: 1.4748 - val_acc: 0.6477\n",
      "Epoch 10/1000\n",
      "450/450 [==============================] - 24s 54ms/step - loss: 0.7752 - acc: 0.7473 - val_loss: 1.5589 - val_acc: 0.6534\n",
      "Epoch 11/1000\n",
      "450/450 [==============================] - 20s 45ms/step - loss: 0.7408 - acc: 0.7631 - val_loss: 1.5088 - val_acc: 0.6193\n",
      "Epoch 12/1000\n",
      "450/450 [==============================] - 26s 57ms/step - loss: 0.6400 - acc: 0.7963 - val_loss: 1.5044 - val_acc: 0.6591\n",
      "Epoch 13/1000\n",
      "450/450 [==============================] - 25s 57ms/step - loss: 0.6011 - acc: 0.8079 - val_loss: 1.7243 - val_acc: 0.6193\n",
      "Epoch 14/1000\n",
      "450/450 [==============================] - 22s 50ms/step - loss: 0.5312 - acc: 0.8183 - val_loss: 1.7458 - val_acc: 0.6591\n",
      "Epoch 15/1000\n",
      "450/450 [==============================] - 22s 48ms/step - loss: 0.5142 - acc: 0.8364 - val_loss: 1.6599 - val_acc: 0.6250\n",
      "Epoch 16/1000\n",
      "450/450 [==============================] - 22s 50ms/step - loss: 0.4682 - acc: 0.8500 - val_loss: 1.7221 - val_acc: 0.6193\n",
      "Epoch 17/1000\n",
      "450/450 [==============================] - 20s 45ms/step - loss: 0.4212 - acc: 0.8668 - val_loss: 1.8839 - val_acc: 0.6250\n",
      "Epoch 18/1000\n",
      "450/450 [==============================] - 22s 49ms/step - loss: 0.3967 - acc: 0.8675 - val_loss: 2.6238 - val_acc: 0.6477\n",
      "Epoch 19/1000\n",
      "450/450 [==============================] - 23s 50ms/step - loss: 0.3996 - acc: 0.8719 - val_loss: 1.8465 - val_acc: 0.6420\n",
      "Epoch 20/1000\n",
      "450/450 [==============================] - 21s 47ms/step - loss: 0.3650 - acc: 0.8912 - val_loss: 1.6101 - val_acc: 0.6193\n",
      "Epoch 21/1000\n",
      "450/450 [==============================] - 20s 44ms/step - loss: 0.3122 - acc: 0.9049 - val_loss: 1.9622 - val_acc: 0.6307\n",
      "Epoch 22/1000\n",
      "450/450 [==============================] - 23s 51ms/step - loss: 0.3213 - acc: 0.9026 - val_loss: 1.6187 - val_acc: 0.6648\n",
      "Epoch 23/1000\n",
      "450/450 [==============================] - 24s 53ms/step - loss: 0.3170 - acc: 0.9031 - val_loss: 2.0164 - val_acc: 0.6193\n",
      "Epoch 24/1000\n",
      "450/450 [==============================] - 23s 52ms/step - loss: 0.3003 - acc: 0.9033 - val_loss: 1.5438 - val_acc: 0.6193\n",
      "Epoch 25/1000\n",
      "450/450 [==============================] - 21s 47ms/step - loss: 0.3114 - acc: 0.9090 - val_loss: 2.1012 - val_acc: 0.6420\n",
      "Epoch 26/1000\n",
      "450/450 [==============================] - 26s 57ms/step - loss: 0.2935 - acc: 0.9142 - val_loss: 1.7437 - val_acc: 0.6307\n",
      "Epoch 27/1000\n",
      "450/450 [==============================] - 21s 46ms/step - loss: 0.2855 - acc: 0.9175 - val_loss: 2.0328 - val_acc: 0.6307\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_model():\n",
    "    inputs = tf.keras.layers.Input(shape=INPUT_SHAPE)    \n",
    "    x = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    stacks = []\n",
    "    for kernel_size in [7, 5, 3]:                \n",
    "        x0 = tf.keras.layers.Conv2D(\n",
    "            32, \n",
    "            kernel_size=(kernel_size,kernel_size), \n",
    "            strides=(1,1),  \n",
    "            padding='same',                                       \n",
    "            activation=None)(x)    \n",
    "        x0 = tf.keras.layers.Activation('relu')(x0)\n",
    "        stacks.append(x0)\n",
    "        \n",
    "    x = tf.keras.layers.concatenate(stacks,axis=-1)\n",
    "    x = tf.keras.layers.MaxPooling2D(2,2, padding='valid')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    \n",
    "    stacks = []\n",
    "    for kernel_size in [5, 3]:                \n",
    "        x0 = tf.keras.layers.Conv2D(\n",
    "            64, \n",
    "            kernel_size=(kernel_size,kernel_size), \n",
    "            strides=(1,1),  \n",
    "            padding='same',                                       \n",
    "            activation=None)(x)    \n",
    "        x0 = tf.keras.layers.Activation('relu')(x0)\n",
    "        stacks.append(x0)\n",
    "        \n",
    "    x = tf.keras.layers.concatenate(stacks,axis=-1)\n",
    "    x = tf.keras.layers.MaxPooling2D(2,2, padding='valid')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        128, \n",
    "        kernel_size=(3,3), \n",
    "        strides=(1,1),  \n",
    "        padding='same',                                       \n",
    "        activation=None)(x)    \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(2,2, padding='valid')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        \n",
    "    x = tf.keras.layers.Flatten()(x) \n",
    "    \n",
    "    x = tf.keras.layers.Dense(256, activation=None)(x) \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES, activation=None)(x) \n",
    "    output = tf.keras.layers.Activation('softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[output])\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "225/225 [==============================] - 78s 348ms/step - loss: 2.8129 - acc: 0.1676 - val_loss: 2.8365 - val_acc: 0.3636\n",
      "Epoch 2/1000\n",
      "225/225 [==============================] - 76s 340ms/step - loss: 2.2693 - acc: 0.3150 - val_loss: 2.1543 - val_acc: 0.4602\n",
      "Epoch 3/1000\n",
      "225/225 [==============================] - 79s 351ms/step - loss: 2.0154 - acc: 0.3829 - val_loss: 1.8296 - val_acc: 0.5114\n",
      "Epoch 4/1000\n",
      "225/225 [==============================] - 81s 360ms/step - loss: 1.7968 - acc: 0.4433 - val_loss: 1.5984 - val_acc: 0.5625\n",
      "Epoch 5/1000\n",
      "225/225 [==============================] - 82s 366ms/step - loss: 1.6842 - acc: 0.4804 - val_loss: 1.4696 - val_acc: 0.5909\n",
      "Epoch 6/1000\n",
      "225/225 [==============================] - 82s 363ms/step - loss: 1.5768 - acc: 0.5103 - val_loss: 1.4825 - val_acc: 0.5511\n",
      "Epoch 7/1000\n",
      "225/225 [==============================] - 80s 357ms/step - loss: 1.4380 - acc: 0.5434 - val_loss: 1.4429 - val_acc: 0.5909\n",
      "Epoch 8/1000\n",
      "225/225 [==============================] - 76s 338ms/step - loss: 1.3470 - acc: 0.5707 - val_loss: 1.3643 - val_acc: 0.6136\n",
      "Epoch 9/1000\n",
      "225/225 [==============================] - 76s 339ms/step - loss: 1.2521 - acc: 0.6007 - val_loss: 1.3303 - val_acc: 0.6136\n",
      "Epoch 10/1000\n",
      "225/225 [==============================] - 76s 338ms/step - loss: 1.1692 - acc: 0.6202 - val_loss: 1.3029 - val_acc: 0.6307\n",
      "Epoch 11/1000\n",
      "225/225 [==============================] - 77s 341ms/step - loss: 1.0975 - acc: 0.6502 - val_loss: 1.3553 - val_acc: 0.6080\n",
      "Epoch 12/1000\n",
      "225/225 [==============================] - 76s 339ms/step - loss: 1.0586 - acc: 0.6557 - val_loss: 1.3069 - val_acc: 0.6080\n",
      "Epoch 13/1000\n",
      "225/225 [==============================] - 76s 337ms/step - loss: 1.0183 - acc: 0.6676 - val_loss: 1.2870 - val_acc: 0.6193\n",
      "Epoch 14/1000\n",
      "225/225 [==============================] - 76s 338ms/step - loss: 0.9451 - acc: 0.6825 - val_loss: 1.3567 - val_acc: 0.6080\n",
      "Epoch 15/1000\n",
      "225/225 [==============================] - 75s 334ms/step - loss: 0.9224 - acc: 0.6878 - val_loss: 1.3120 - val_acc: 0.6080\n",
      "Epoch 16/1000\n",
      "225/225 [==============================] - 75s 335ms/step - loss: 0.8670 - acc: 0.7213 - val_loss: 1.3650 - val_acc: 0.6364\n",
      "Epoch 17/1000\n",
      "225/225 [==============================] - 76s 336ms/step - loss: 0.8352 - acc: 0.7286 - val_loss: 1.4855 - val_acc: 0.6136\n",
      "Epoch 18/1000\n",
      "225/225 [==============================] - 76s 336ms/step - loss: 0.8218 - acc: 0.7319 - val_loss: 1.2980 - val_acc: 0.6136\n",
      "Epoch 19/1000\n",
      "225/225 [==============================] - 76s 338ms/step - loss: 0.7911 - acc: 0.7389 - val_loss: 1.3574 - val_acc: 0.6136\n",
      "Epoch 20/1000\n",
      "225/225 [==============================] - 78s 345ms/step - loss: 0.7218 - acc: 0.7525 - val_loss: 1.4707 - val_acc: 0.6364\n",
      "Epoch 21/1000\n",
      "225/225 [==============================] - 77s 341ms/step - loss: 0.7293 - acc: 0.7630 - val_loss: 1.3950 - val_acc: 0.6364\n",
      "Epoch 22/1000\n",
      "225/225 [==============================] - 75s 335ms/step - loss: 0.7145 - acc: 0.7616 - val_loss: 1.3859 - val_acc: 0.6136\n",
      "Epoch 23/1000\n",
      "225/225 [==============================] - 73s 327ms/step - loss: 0.6715 - acc: 0.7792 - val_loss: 1.4122 - val_acc: 0.6023\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential() \n",
    "    model.add(tf.keras.layers.Conv2D(16, kernel_size=(7,7), strides=(3,3),  \n",
    "                            padding='same', activation='relu', input_shape=INPUT_SHAPE))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, padding='same'))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(5,5), padding='same', activation='relu'))        \n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, padding='same'))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(64, (5,5), padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, padding='same'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.125))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(128, (5,5), padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())                \n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu', name='dense_2_512'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax', name='output'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer='rmsprop', \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 2.8709 - acc: 0.1103 - val_loss: 2.4940 - val_acc: 0.1989\n",
      "Epoch 2/1000\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 2.3486 - acc: 0.2643 - val_loss: 2.1257 - val_acc: 0.3239\n",
      "Epoch 3/1000\n",
      "225/225 [==============================] - 4s 20ms/step - loss: 2.0257 - acc: 0.3671 - val_loss: 2.0584 - val_acc: 0.3864\n",
      "Epoch 4/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 1.8254 - acc: 0.4257 - val_loss: 1.8259 - val_acc: 0.4943\n",
      "Epoch 5/1000\n",
      "225/225 [==============================] - 4s 20ms/step - loss: 1.6348 - acc: 0.4779 - val_loss: 1.7840 - val_acc: 0.4830\n",
      "Epoch 6/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 1.4563 - acc: 0.5352 - val_loss: 1.7792 - val_acc: 0.4773\n",
      "Epoch 7/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 1.3071 - acc: 0.5807 - val_loss: 1.8117 - val_acc: 0.5057\n",
      "Epoch 8/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 1.1571 - acc: 0.6213 - val_loss: 1.8888 - val_acc: 0.5455\n",
      "Epoch 9/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.9867 - acc: 0.6719 - val_loss: 1.7259 - val_acc: 0.5625\n",
      "Epoch 10/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.8641 - acc: 0.7061 - val_loss: 1.9153 - val_acc: 0.5341\n",
      "Epoch 11/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.7429 - acc: 0.7611 - val_loss: 1.8997 - val_acc: 0.5795\n",
      "Epoch 12/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.6109 - acc: 0.7950 - val_loss: 1.8679 - val_acc: 0.5795\n",
      "Epoch 13/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.5055 - acc: 0.8250 - val_loss: 2.2561 - val_acc: 0.5284\n",
      "Epoch 14/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.4134 - acc: 0.8581 - val_loss: 2.3952 - val_acc: 0.5455\n",
      "Epoch 15/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.3640 - acc: 0.8744 - val_loss: 2.5897 - val_acc: 0.5511\n",
      "Epoch 16/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.2891 - acc: 0.9078 - val_loss: 2.4962 - val_acc: 0.5795\n",
      "Epoch 17/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.2361 - acc: 0.9189 - val_loss: 2.8124 - val_acc: 0.5341\n",
      "Epoch 18/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.2125 - acc: 0.9281 - val_loss: 2.8239 - val_acc: 0.5511\n",
      "Epoch 19/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.1861 - acc: 0.9408 - val_loss: 2.8668 - val_acc: 0.5511\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential() \n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), strides=(3,3),  \n",
    "                            padding='same', activation='relu', input_shape=INPUT_SHAPE))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), padding='same', activation='relu'))        \n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2))      \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), padding='same', activation='relu'))        \n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2))   \n",
    "    model.add(tf.keras.layers.Dropout(0.33))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())                \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu', name='dense_2_512'))\n",
    "    model.add(tf.keras.layers.Dropout(0.33))\n",
    "    \n",
    "#     model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dropout(0.33))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax', name='output'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer='rmsprop', \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 2.9253 - acc: 0.1126 - val_loss: 2.4768 - val_acc: 0.2500\n",
      "Epoch 2/1000\n",
      "225/225 [==============================] - 4s 16ms/step - loss: 2.4946 - acc: 0.2264 - val_loss: 2.1238 - val_acc: 0.3750\n",
      "Epoch 3/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 2.2695 - acc: 0.2923 - val_loss: 1.9798 - val_acc: 0.3580\n",
      "Epoch 4/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 2.1537 - acc: 0.3220 - val_loss: 1.9027 - val_acc: 0.4545\n",
      "Epoch 5/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 2.0305 - acc: 0.3713 - val_loss: 1.8102 - val_acc: 0.4261\n",
      "Epoch 6/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.9485 - acc: 0.3893 - val_loss: 1.7647 - val_acc: 0.4432\n",
      "Epoch 7/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.8978 - acc: 0.4000 - val_loss: 1.7754 - val_acc: 0.4432\n",
      "Epoch 8/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.8175 - acc: 0.4317 - val_loss: 1.7484 - val_acc: 0.4489\n",
      "Epoch 9/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.7587 - acc: 0.4427 - val_loss: 1.7526 - val_acc: 0.4318\n",
      "Epoch 10/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.7169 - acc: 0.4585 - val_loss: 1.7008 - val_acc: 0.5000\n",
      "Epoch 11/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.6646 - acc: 0.4758 - val_loss: 1.7251 - val_acc: 0.4659\n",
      "Epoch 12/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.6057 - acc: 0.4969 - val_loss: 1.7137 - val_acc: 0.5341\n",
      "Epoch 13/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.5543 - acc: 0.5065 - val_loss: 1.6717 - val_acc: 0.4943\n",
      "Epoch 14/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.5248 - acc: 0.5221 - val_loss: 1.6972 - val_acc: 0.5000\n",
      "Epoch 15/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.4714 - acc: 0.5194 - val_loss: 1.6988 - val_acc: 0.4886\n",
      "Epoch 16/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.4328 - acc: 0.5337 - val_loss: 1.7379 - val_acc: 0.4830\n",
      "Epoch 17/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.3710 - acc: 0.5568 - val_loss: 1.7341 - val_acc: 0.5227\n",
      "Epoch 18/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.3304 - acc: 0.5736 - val_loss: 1.6625 - val_acc: 0.5511\n",
      "Epoch 19/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.3054 - acc: 0.5772 - val_loss: 1.6976 - val_acc: 0.5284\n",
      "Epoch 20/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.2594 - acc: 0.5854 - val_loss: 1.6964 - val_acc: 0.5057\n",
      "Epoch 21/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.2401 - acc: 0.5991 - val_loss: 1.6818 - val_acc: 0.5398\n",
      "Epoch 22/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.2161 - acc: 0.6163 - val_loss: 1.7270 - val_acc: 0.5284\n",
      "Epoch 23/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.1632 - acc: 0.6164 - val_loss: 1.7216 - val_acc: 0.5227\n",
      "Epoch 24/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.1608 - acc: 0.6184 - val_loss: 1.7040 - val_acc: 0.5511\n",
      "Epoch 25/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.1120 - acc: 0.6306 - val_loss: 1.7159 - val_acc: 0.5625\n",
      "Epoch 26/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.0617 - acc: 0.6556 - val_loss: 1.8203 - val_acc: 0.5284\n",
      "Epoch 27/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.0647 - acc: 0.6502 - val_loss: 1.8528 - val_acc: 0.5398\n",
      "Epoch 28/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.0606 - acc: 0.6471 - val_loss: 1.8195 - val_acc: 0.5568\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential() \n",
    "    model.add(tf.keras.layers.Conv2D(16, kernel_size=(5,5), strides=(3,3),  \n",
    "                            padding='same', activation='relu', input_shape=INPUT_SHAPE))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, padding='same'))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), padding='same', activation='relu'))        \n",
    "    model.add(tf.keras.layers.MaxPooling2D(2,2, padding='same'))   \n",
    "    model.add(tf.keras.layers.Dropout(0.33))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())                \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu', name='dense_2_512'))\n",
    "    model.add(tf.keras.layers.Dropout(0.33))\n",
    "    \n",
    "#     model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dropout(0.33))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax', name='output'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer='rmsprop', \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "225/225 [==============================] - 4s 20ms/step - loss: 2.6679 - acc: 0.1829 - val_loss: 2.1610 - val_acc: 0.3239\n",
      "Epoch 2/1000\n",
      "225/225 [==============================] - 4s 17ms/step - loss: 2.2071 - acc: 0.3184 - val_loss: 1.8687 - val_acc: 0.4602\n",
      "Epoch 3/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.9675 - acc: 0.3945 - val_loss: 1.7776 - val_acc: 0.4659\n",
      "Epoch 4/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.8038 - acc: 0.4513 - val_loss: 1.7342 - val_acc: 0.4602\n",
      "Epoch 5/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 1.6667 - acc: 0.4898 - val_loss: 1.6631 - val_acc: 0.5114\n",
      "Epoch 6/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.5413 - acc: 0.5140 - val_loss: 1.6376 - val_acc: 0.5057\n",
      "Epoch 7/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.4631 - acc: 0.5408 - val_loss: 1.6109 - val_acc: 0.5284\n",
      "Epoch 8/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.3879 - acc: 0.5593 - val_loss: 1.6443 - val_acc: 0.5227\n",
      "Epoch 9/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.3095 - acc: 0.5912 - val_loss: 1.6369 - val_acc: 0.5341\n",
      "Epoch 10/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.2117 - acc: 0.6142 - val_loss: 1.6055 - val_acc: 0.5057\n",
      "Epoch 11/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.1446 - acc: 0.6259 - val_loss: 1.6319 - val_acc: 0.5284\n",
      "Epoch 12/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 1.0691 - acc: 0.6554 - val_loss: 1.6432 - val_acc: 0.5398\n",
      "Epoch 13/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.9891 - acc: 0.6824 - val_loss: 1.6752 - val_acc: 0.5625\n",
      "Epoch 14/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.9368 - acc: 0.7008 - val_loss: 1.6640 - val_acc: 0.5284\n",
      "Epoch 15/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.8868 - acc: 0.7123 - val_loss: 1.6634 - val_acc: 0.5568\n",
      "Epoch 16/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 0.8329 - acc: 0.7290 - val_loss: 1.6740 - val_acc: 0.5511\n",
      "Epoch 17/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.7610 - acc: 0.7459 - val_loss: 1.7718 - val_acc: 0.5000\n",
      "Epoch 18/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 0.7178 - acc: 0.7644 - val_loss: 1.7786 - val_acc: 0.5455\n",
      "Epoch 19/1000\n",
      "225/225 [==============================] - 3s 14ms/step - loss: 0.7077 - acc: 0.7641 - val_loss: 1.7568 - val_acc: 0.5455\n",
      "Epoch 20/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.6554 - acc: 0.7861 - val_loss: 1.9291 - val_acc: 0.5227\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.)\n",
    "    \n",
    "# create an iterator for the training data \n",
    "train_generator = datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=NUM_TRAIN_FILES, \n",
    "    color_mode='grayscale')\n",
    "\n",
    "# create an iterator for the validation data \n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    VALID_DIR,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=NUM_VALID_FILES, \n",
    "    color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_generator.next()\n",
    "x_test, y_test = validation_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 99.67%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 65.91%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export weights\n",
    "The type of each entry in array is given by -dataType. The number of entries is equal to:\n",
    "\n",
    "inputFeatureChannels outputFeatureChannels kernelHeight kernelWidth*\n",
    "\n",
    "The layout of filter weight is as a 4D tensor (array) weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ inputChannels / groups ]\n",
    "\n",
    "Note: For binary-convolutions the layout of the weights are: weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ floor((inputChannels/groups)+31) / 32 ] with each 32 sub input feature channel index specified in machine byte order, so that for example the 13th feature channel bit can be extracted using bitmask = (1U << 13)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_conv_weights(name, wts_coef, bias_coef):\n",
    "    print(\"Exporting weights for {}\\n\\t{}\\n\\t{}\".format(name, \n",
    "          os.path.join('exports', \"{}_conv.data\".format(name)), \n",
    "          os.path.join('exports', \"{}_bias.data\".format(name))))\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # [kernel_width, kernel_height, input_feature_channels, output_feature_channels]\n",
    "    print(\"\\tOriginal weights shape {}\".format(wts_coef.shape))\n",
    "    if bias_coef is not None:\n",
    "        # [output_feature_channels]\n",
    "        print(\"\\tOriginal bias shape {}\".format(bias_coef.shape))\n",
    "    \n",
    "    # [output_feature_channels, kernel_width, kernel_height, input_feature_channels]\n",
    "    wts_coef = wts_coef.transpose(3, 0, 1, 2)\n",
    "    print(\"\\tReshaped weights shape {}\".format(wts_coef.shape))    \n",
    "    wts_coef.tofile(os.path.join('exports', \"{}_conv.data\".format(name)))    \n",
    "    \n",
    "    if bias_coef is not None:\n",
    "        bias_coef = np.squeeze(bias_coef)\n",
    "        print(\"\\tReshaped bias_coef shape {}\".format(bias_coef.shape))    \n",
    "        bias_coef.tofile(os.path.join('exports', \"{}_bias.data\".format(name)))    \n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dense_weights(name, wts_coef, bias_coef, kernel_width, kernel_height):\n",
    "    \"\"\"\n",
    "    A fully connected layer in a Convolutional Neural Network (CNN) is one where every input channel is connected \n",
    "    to every output channel. The kernel width is equal to the width of the source image, and the \n",
    "    kernel height is equal to the height of the source image. The width and height of the output is 1 x 1.\n",
    "    \n",
    "    A fully connected layer takes an MPSImage object with dimensions \n",
    "    source.width x source.height x Ni, convolves it with Weights[No][source.width][source.height][Ni], \n",
    "    and produces a 1 x 1 x No output.\n",
    "    \n",
    "    Thus, the following conditions must be true:\n",
    "    - kernelWidth == source.width\n",
    "    - kernelHeight == source.height\n",
    "    - clipRect.size.width == 1\n",
    "    - clipRect.size.height == 1\n",
    "    \n",
    "    You can think of a fully connected layer as a matrix multiplication where the image is \n",
    "    flattened into a vector of length source.width*source.height*Ni, and the weights are arranged in a \n",
    "    matrix of dimension No x (source.width*source.height*Ni) to produce an output vector of length No.\n",
    "    \n",
    "    The value of the strideInPixelsX, strideInPixelsY, and groups properties must be 1. \n",
    "    The offset property is not applicable and it is ignored. Because the clip rectangle is \n",
    "    clamped to the destination image bounds, if the destination is 1 x 1, you do not need to set the \n",
    "    clipRect property.\n",
    "    \"\"\"\n",
    "    print(\"Exporting weights for {}\\n\\t{}\\n\\t{}\".format(name, \n",
    "          os.path.join('exports', \"{}_conv.data\".format(name)), \n",
    "          os.path.join('exports', \"{}_bias.data\".format(name))))\n",
    "        \n",
    "    input_feature_channels = int(wts_coef.shape[0] / kernel_width / kernel_height) \n",
    "    output_feature_channels = wts_coef.shape[-1]            \n",
    "    \n",
    "    # [kernel_width, kernel_height, input_feature_channels, output_feature_channels]\n",
    "    print(\"\\tOriginal weights shape {}\".format(wts_coef.shape))\n",
    "    \n",
    "    #wts_coef = np.reshape(wts_coef, [kernel_width, kernel_height, input_feature_channels, output_feature_channels])    \n",
    "    wts_coef = np.reshape(wts_coef, [kernel_width, kernel_height, -1, output_feature_channels])    \n",
    "        \n",
    "    if bias_coef is not None:\n",
    "        # [output_feature_channels]\n",
    "        print(\"\\tOriginal bias shape {}\".format(bias_coef.shape))\n",
    "    \n",
    "    # [output_feature_channels, kernel_width, kernel_height, input_feature_channels]\n",
    "    wts_coef = wts_coef.transpose(3, 0, 1, 2)\n",
    "    print(\"\\tReshaped weights shape {}\".format(wts_coef.shape))    \n",
    "    wts_coef.tofile(os.path.join('exports', \"{}_conv.data\".format(name)))    \n",
    "    \n",
    "    if bias_coef is not None:\n",
    "        bias_coef = np.squeeze(bias_coef)\n",
    "        print(\"\\tReshaped bias_coef shape {}\".format(bias_coef.shape))    \n",
    "        bias_coef.tofile(os.path.join('exports', \"{}_bias.data\".format(name)))   \n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatted_input_kernel_width = None\n",
    "flatted_input_kernel_height = None\n",
    "\n",
    "for layer in model.layers:        \n",
    "    if \"flatten\" in layer.name:\n",
    "        flatted_input_kernel_width = layer.input_shape[1] # None, 14, 14, 64\n",
    "        flatted_input_kernel_height = layer.input_shape[2] # None, 14, 14, 64\n",
    "        print(\"flatted_input_kernel_width {} flatted_input_kernel_height {}\".format(\n",
    "            flatted_input_kernel_width, flatted_input_kernel_height))\n",
    "        \n",
    "    if len(layer.get_weights()) > 0:        \n",
    "        name = layer.name         \n",
    "        wts = layer.get_weights()\n",
    "        \n",
    "        if \"conv\" in name:            \n",
    "            export_conv_weights(layer.name, wts[0], wts[1] if len(wts) == 2 else None)        \n",
    "        else:\n",
    "            export_dense_weights(layer.name, wts[0], wts[1] if len(wts) == 2 else None, \n",
    "                                flatted_input_kernel_width, flatted_input_kernel_height)        \n",
    "            # after the initial pass (from cnn to fcn); flattern the kernel down to 1x1 \n",
    "            # i.e. update the flatted_input_kernel_DIM to have the kernel width and height of 1 \n",
    "            flatted_input_kernel_width, flatted_input_kernel_height = 1, 1             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    https://www.iioab.org/articles/IIOABJ_7.S5_337-341.pdf\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(96, \n",
    "                                     kernel_size=(10,10), \n",
    "                                     strides=(1,1),  \n",
    "                                     padding='valid',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(192, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='valid', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "    model.add(tf.keras.layers.Dropout(0.35))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(192, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='valid', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "        \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(192, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.35))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES,activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=\"sgd\", # tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 55, 55, 96)        9696      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 55, 55, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 23, 23, 192)       460992    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 23, 23, 192)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 11, 11, 192)       0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 11, 11, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 9, 9, 192)         331968    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 9, 9, 192)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 15552)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 192)               2986176   \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 22)                4246      \n",
      "_________________________________________________________________\n",
      "output (Activation)          (None, 22)                0         \n",
      "=================================================================\n",
      "Total params: 3,793,078\n",
      "Trainable params: 3,793,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "225/225 [==============================] - 53s 235ms/step - loss: 3.0778 - acc: 0.0553 - val_loss: 3.0429 - val_acc: 0.1364\n",
      "Epoch 2/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 2.9464 - acc: 0.1145 - val_loss: 2.6928 - val_acc: 0.1989\n",
      "Epoch 3/1000\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 2.6573 - acc: 0.2001 - val_loss: 2.4005 - val_acc: 0.3011\n",
      "Epoch 4/1000\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 2.4546 - acc: 0.2484 - val_loss: 2.1870 - val_acc: 0.3693\n",
      "Epoch 5/1000\n",
      "225/225 [==============================] - 48s 212ms/step - loss: 2.3132 - acc: 0.2868 - val_loss: 2.0689 - val_acc: 0.3920\n",
      "Epoch 6/1000\n",
      "225/225 [==============================] - 48s 211ms/step - loss: 2.2147 - acc: 0.3164 - val_loss: 1.9824 - val_acc: 0.4148\n",
      "Epoch 7/1000\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 2.1061 - acc: 0.3528 - val_loss: 1.9519 - val_acc: 0.4091\n",
      "Epoch 8/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 2.0262 - acc: 0.3724 - val_loss: 1.8855 - val_acc: 0.4261\n",
      "Epoch 9/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 1.9546 - acc: 0.3989 - val_loss: 1.8288 - val_acc: 0.4489\n",
      "Epoch 10/1000\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 1.8709 - acc: 0.4143 - val_loss: 1.8184 - val_acc: 0.4148\n",
      "Epoch 11/1000\n",
      "225/225 [==============================] - 50s 220ms/step - loss: 1.8135 - acc: 0.4392 - val_loss: 1.8040 - val_acc: 0.4716\n",
      "Epoch 12/1000\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 1.7353 - acc: 0.4593 - val_loss: 1.7699 - val_acc: 0.4886\n",
      "Epoch 13/1000\n",
      "225/225 [==============================] - 50s 223ms/step - loss: 1.6673 - acc: 0.4828 - val_loss: 1.7341 - val_acc: 0.4830\n",
      "Epoch 14/1000\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 1.6040 - acc: 0.4988 - val_loss: 1.7696 - val_acc: 0.4545\n",
      "Epoch 15/1000\n",
      "225/225 [==============================] - 52s 230ms/step - loss: 1.5424 - acc: 0.5283 - val_loss: 1.7596 - val_acc: 0.5000\n",
      "Epoch 16/1000\n",
      "225/225 [==============================] - 51s 226ms/step - loss: 1.4865 - acc: 0.5348 - val_loss: 1.7462 - val_acc: 0.5000\n",
      "Epoch 17/1000\n",
      "225/225 [==============================] - 50s 220ms/step - loss: 1.4114 - acc: 0.5507 - val_loss: 1.7562 - val_acc: 0.5114\n",
      "Epoch 18/1000\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 1.3431 - acc: 0.5774 - val_loss: 1.7774 - val_acc: 0.5114\n",
      "Epoch 19/1000\n",
      "225/225 [==============================] - 50s 220ms/step - loss: 1.3393 - acc: 0.5755 - val_loss: 1.7598 - val_acc: 0.5227\n",
      "Epoch 20/1000\n",
      "225/225 [==============================] - 48s 213ms/step - loss: 1.2432 - acc: 0.6048 - val_loss: 1.7609 - val_acc: 0.4830\n",
      "Epoch 21/1000\n",
      "225/225 [==============================] - 51s 226ms/step - loss: 1.2027 - acc: 0.6193 - val_loss: 1.8069 - val_acc: 0.4830\n",
      "Epoch 22/1000\n",
      "225/225 [==============================] - 50s 224ms/step - loss: 1.1417 - acc: 0.6328 - val_loss: 1.8277 - val_acc: 0.5000\n",
      "Epoch 23/1000\n",
      "225/225 [==============================] - 51s 228ms/step - loss: 1.0933 - acc: 0.6499 - val_loss: 1.7890 - val_acc: 0.5341\n"
     ]
    }
   ],
   "source": [
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(7,7), \n",
    "                                     strides=(2,2),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))           \n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(3,3))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))        \n",
    "    model.add(tf.keras.layers.Dropout(0.4)) \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(32, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.3))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_63 (Conv2D)           (None, 64, 64, 32)        1600      \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_64 (Conv2D)           (None, 64, 64, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_65 (Conv2D)           (None, 21, 21, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_66 (Conv2D)           (None, 10, 10, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 32)                25632     \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 22)                726       \n",
      "_________________________________________________________________\n",
      "output (Activation)          (None, 22)                0         \n",
      "=================================================================\n",
      "Total params: 88,470\n",
      "Trainable params: 88,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "450/450 [==============================] - 60s 133ms/step - loss: 2.9245 - acc: 0.1098 - val_loss: 2.4482 - val_acc: 0.2784\n",
      "Epoch 2/1000\n",
      "450/450 [==============================] - 59s 131ms/step - loss: 2.4531 - acc: 0.2420 - val_loss: 2.1753 - val_acc: 0.4034\n",
      "Epoch 3/1000\n",
      "450/450 [==============================] - 57s 126ms/step - loss: 2.2052 - acc: 0.3226 - val_loss: 1.9569 - val_acc: 0.4716\n",
      "Epoch 4/1000\n",
      "450/450 [==============================] - 57s 128ms/step - loss: 2.0291 - acc: 0.3707 - val_loss: 1.6818 - val_acc: 0.5284\n",
      "Epoch 5/1000\n",
      "450/450 [==============================] - 57s 127ms/step - loss: 1.8622 - acc: 0.4229 - val_loss: 1.6938 - val_acc: 0.5227\n",
      "Epoch 6/1000\n",
      "450/450 [==============================] - 60s 133ms/step - loss: 1.7634 - acc: 0.4637 - val_loss: 1.6022 - val_acc: 0.5739\n",
      "Epoch 7/1000\n",
      "450/450 [==============================] - 58s 130ms/step - loss: 1.6562 - acc: 0.4853 - val_loss: 1.4138 - val_acc: 0.6250\n",
      "Epoch 8/1000\n",
      "450/450 [==============================] - 58s 129ms/step - loss: 1.6351 - acc: 0.4879 - val_loss: 1.4070 - val_acc: 0.6534\n",
      "Epoch 9/1000\n",
      "450/450 [==============================] - 58s 129ms/step - loss: 1.5212 - acc: 0.5211 - val_loss: 1.3859 - val_acc: 0.6477\n",
      "Epoch 10/1000\n",
      "450/450 [==============================] - 58s 129ms/step - loss: 1.4752 - acc: 0.5501 - val_loss: 1.2851 - val_acc: 0.6875\n",
      "Epoch 11/1000\n",
      "450/450 [==============================] - 58s 129ms/step - loss: 1.3827 - acc: 0.5676 - val_loss: 1.3190 - val_acc: 0.6534\n",
      "Epoch 12/1000\n",
      "450/450 [==============================] - 57s 128ms/step - loss: 1.3595 - acc: 0.5690 - val_loss: 1.2903 - val_acc: 0.6818\n",
      "Epoch 13/1000\n",
      "450/450 [==============================] - 59s 130ms/step - loss: 1.2975 - acc: 0.5824 - val_loss: 1.2893 - val_acc: 0.6477\n",
      "Epoch 14/1000\n",
      "450/450 [==============================] - 59s 132ms/step - loss: 1.2863 - acc: 0.5887 - val_loss: 1.2228 - val_acc: 0.6420\n",
      "Epoch 15/1000\n",
      "450/450 [==============================] - 57s 128ms/step - loss: 1.2355 - acc: 0.6122 - val_loss: 1.1436 - val_acc: 0.6534\n",
      "Epoch 16/1000\n",
      "450/450 [==============================] - 58s 129ms/step - loss: 1.2148 - acc: 0.6243 - val_loss: 1.2131 - val_acc: 0.6761\n",
      "Epoch 17/1000\n",
      "450/450 [==============================] - 59s 130ms/step - loss: 1.1982 - acc: 0.6188 - val_loss: 1.2372 - val_acc: 0.6477\n",
      "Epoch 18/1000\n",
      "450/450 [==============================] - 60s 133ms/step - loss: 1.1580 - acc: 0.6354 - val_loss: 1.1899 - val_acc: 0.6591\n",
      "Epoch 19/1000\n",
      "450/450 [==============================] - 58s 129ms/step - loss: 1.1299 - acc: 0.6393 - val_loss: 1.2563 - val_acc: 0.6420\n",
      "Epoch 20/1000\n",
      "450/450 [==============================] - 59s 131ms/step - loss: 1.1387 - acc: 0.6390 - val_loss: 1.2186 - val_acc: 0.6705\n"
     ]
    }
   ],
   "source": [
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (128,128)\n",
    "INPUT_SHAPE = (128,128,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(7,7), \n",
    "                                     strides=(2,2),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))           \n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))        \n",
    "    model.add(tf.keras.layers.Dropout(0.4)) \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(32, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.3))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "56/56 [==============================] - 53s 948ms/step - loss: 2.9346 - acc: 0.1220 - val_loss: 2.6917 - val_acc: 0.2500\n",
      "Epoch 2/1000\n",
      "56/56 [==============================] - 52s 929ms/step - loss: 2.5938 - acc: 0.2317 - val_loss: 2.2908 - val_acc: 0.3516\n",
      "Epoch 3/1000\n",
      "56/56 [==============================] - 51s 915ms/step - loss: 2.3669 - acc: 0.2942 - val_loss: 2.1253 - val_acc: 0.4609\n",
      "Epoch 4/1000\n",
      "56/56 [==============================] - 51s 919ms/step - loss: 2.1921 - acc: 0.3366 - val_loss: 1.9819 - val_acc: 0.4688\n",
      "Epoch 5/1000\n",
      "56/56 [==============================] - 51s 907ms/step - loss: 2.0813 - acc: 0.3659 - val_loss: 1.7306 - val_acc: 0.5312\n",
      "Epoch 6/1000\n",
      "56/56 [==============================] - 51s 919ms/step - loss: 1.9778 - acc: 0.4036 - val_loss: 1.5973 - val_acc: 0.5391\n",
      "Epoch 7/1000\n",
      "56/56 [==============================] - 51s 906ms/step - loss: 1.8926 - acc: 0.4161 - val_loss: 1.6025 - val_acc: 0.5625\n",
      "Epoch 8/1000\n",
      "56/56 [==============================] - 49s 879ms/step - loss: 1.7822 - acc: 0.4465 - val_loss: 1.5320 - val_acc: 0.5625\n",
      "Epoch 9/1000\n",
      "56/56 [==============================] - 52s 921ms/step - loss: 1.7252 - acc: 0.4671 - val_loss: 1.4967 - val_acc: 0.5703\n",
      "Epoch 10/1000\n",
      "56/56 [==============================] - 50s 901ms/step - loss: 1.6128 - acc: 0.5028 - val_loss: 1.4015 - val_acc: 0.6172\n",
      "Epoch 11/1000\n",
      "56/56 [==============================] - 51s 918ms/step - loss: 1.6494 - acc: 0.4945 - val_loss: 1.3893 - val_acc: 0.6016\n",
      "Epoch 12/1000\n",
      "56/56 [==============================] - 51s 914ms/step - loss: 1.5395 - acc: 0.5174 - val_loss: 1.3733 - val_acc: 0.6016\n",
      "Epoch 13/1000\n",
      "56/56 [==============================] - 50s 901ms/step - loss: 1.5177 - acc: 0.5225 - val_loss: 1.4943 - val_acc: 0.5703\n",
      "Epoch 14/1000\n",
      "56/56 [==============================] - 52s 922ms/step - loss: 1.4484 - acc: 0.5484 - val_loss: 1.3376 - val_acc: 0.6016\n",
      "Epoch 15/1000\n",
      "56/56 [==============================] - 51s 904ms/step - loss: 1.4016 - acc: 0.5621 - val_loss: 1.2864 - val_acc: 0.6172\n",
      "Epoch 16/1000\n",
      "56/56 [==============================] - 51s 914ms/step - loss: 1.3609 - acc: 0.5671 - val_loss: 1.2792 - val_acc: 0.6016\n",
      "Epoch 17/1000\n",
      "56/56 [==============================] - 52s 937ms/step - loss: 1.3326 - acc: 0.5769 - val_loss: 1.2842 - val_acc: 0.6250\n",
      "Epoch 18/1000\n",
      "56/56 [==============================] - 54s 963ms/step - loss: 1.2576 - acc: 0.5974 - val_loss: 1.1952 - val_acc: 0.6406\n",
      "Epoch 19/1000\n",
      "56/56 [==============================] - 50s 892ms/step - loss: 1.2789 - acc: 0.5815 - val_loss: 1.2187 - val_acc: 0.6250\n",
      "Epoch 20/1000\n",
      "56/56 [==============================] - 52s 921ms/step - loss: 1.2219 - acc: 0.5993 - val_loss: 1.2896 - val_acc: 0.6016\n",
      "Epoch 21/1000\n",
      "56/56 [==============================] - 50s 902ms/step - loss: 1.1882 - acc: 0.6203 - val_loss: 1.1826 - val_acc: 0.6406\n",
      "Epoch 22/1000\n",
      "56/56 [==============================] - 51s 914ms/step - loss: 1.1780 - acc: 0.6266 - val_loss: 1.2448 - val_acc: 0.6484\n",
      "Epoch 23/1000\n",
      "56/56 [==============================] - 52s 927ms/step - loss: 1.1955 - acc: 0.6213 - val_loss: 1.2438 - val_acc: 0.6250\n",
      "Epoch 24/1000\n",
      "56/56 [==============================] - 52s 922ms/step - loss: 1.0912 - acc: 0.6419 - val_loss: 1.1907 - val_acc: 0.6562\n",
      "Epoch 25/1000\n",
      "56/56 [==============================] - 52s 925ms/step - loss: 1.1076 - acc: 0.6313 - val_loss: 1.2027 - val_acc: 0.6484\n",
      "Epoch 26/1000\n",
      "56/56 [==============================] - 49s 878ms/step - loss: 1.0811 - acc: 0.6465 - val_loss: 1.2052 - val_acc: 0.6719\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (128,128)\n",
    "INPUT_SHAPE = (128,128,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(7,7), \n",
    "                                     strides=(2,2),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu')) \n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "              \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Dropout(0.25)) \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(32, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.25))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "112/112 [==============================] - 127s 1s/step - loss: 3.0368 - acc: 0.0882 - val_loss: 2.6109 - val_acc: 0.2313\n",
      "Epoch 2/1000\n",
      "112/112 [==============================] - 127s 1s/step - loss: 2.5311 - acc: 0.2525 - val_loss: 2.0425 - val_acc: 0.4125\n",
      "Epoch 3/1000\n",
      "112/112 [==============================] - 125s 1s/step - loss: 2.1095 - acc: 0.3657 - val_loss: 1.7335 - val_acc: 0.4813\n",
      "Epoch 4/1000\n",
      "112/112 [==============================] - 125s 1s/step - loss: 1.8208 - acc: 0.4592 - val_loss: 1.5732 - val_acc: 0.5188\n",
      "Epoch 5/1000\n",
      "112/112 [==============================] - 121s 1s/step - loss: 1.5987 - acc: 0.5054 - val_loss: 1.5318 - val_acc: 0.5687\n",
      "Epoch 6/1000\n",
      "112/112 [==============================] - 121s 1s/step - loss: 1.4283 - acc: 0.5574 - val_loss: 1.4741 - val_acc: 0.5625\n",
      "Epoch 7/1000\n",
      "112/112 [==============================] - 121s 1s/step - loss: 1.2236 - acc: 0.6106 - val_loss: 1.4337 - val_acc: 0.5875\n",
      "Epoch 8/1000\n",
      "112/112 [==============================] - 121s 1s/step - loss: 1.0632 - acc: 0.6643 - val_loss: 1.3292 - val_acc: 0.6375\n",
      "Epoch 9/1000\n",
      "112/112 [==============================] - 122s 1s/step - loss: 0.9563 - acc: 0.6822 - val_loss: 1.3495 - val_acc: 0.6375\n",
      "Epoch 10/1000\n",
      "112/112 [==============================] - 121s 1s/step - loss: 0.8527 - acc: 0.7260 - val_loss: 1.4013 - val_acc: 0.6438\n",
      "Epoch 11/1000\n",
      "112/112 [==============================] - 118s 1s/step - loss: 0.7655 - acc: 0.7468 - val_loss: 1.4345 - val_acc: 0.6250\n",
      "Epoch 12/1000\n",
      "112/112 [==============================] - 117s 1s/step - loss: 0.6887 - acc: 0.7688 - val_loss: 1.5922 - val_acc: 0.6312\n",
      "Epoch 13/1000\n",
      "112/112 [==============================] - 123s 1s/step - loss: 0.6584 - acc: 0.7752 - val_loss: 1.4898 - val_acc: 0.6188\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (32,32)\n",
    "INPUT_SHAPE = (32,32,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3), \n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))           \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(64, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(64, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(512, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.3))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "112/112 [==============================] - 10s 91ms/step - loss: 2.4908 - acc: 0.2506 - val_loss: 2.0344 - val_acc: 0.3563\n",
      "Epoch 2/1000\n",
      "112/112 [==============================] - 9s 78ms/step - loss: 1.9799 - acc: 0.3890 - val_loss: 1.8709 - val_acc: 0.4437\n",
      "Epoch 3/1000\n",
      "112/112 [==============================] - 8s 72ms/step - loss: 1.7059 - acc: 0.4682 - val_loss: 1.8202 - val_acc: 0.4562\n",
      "Epoch 4/1000\n",
      "112/112 [==============================] - 8s 71ms/step - loss: 1.4090 - acc: 0.5593 - val_loss: 2.0390 - val_acc: 0.4500\n",
      "Epoch 5/1000\n",
      "112/112 [==============================] - 8s 72ms/step - loss: 1.1566 - acc: 0.6319 - val_loss: 1.8169 - val_acc: 0.5000\n",
      "Epoch 6/1000\n",
      "112/112 [==============================] - 8s 72ms/step - loss: 0.8283 - acc: 0.7335 - val_loss: 1.9625 - val_acc: 0.4750\n",
      "Epoch 7/1000\n",
      "112/112 [==============================] - 8s 73ms/step - loss: 0.5617 - acc: 0.8252 - val_loss: 2.4491 - val_acc: 0.4938\n",
      "Epoch 8/1000\n",
      "112/112 [==============================] - 8s 73ms/step - loss: 0.3436 - acc: 0.8878 - val_loss: 2.7456 - val_acc: 0.5125\n",
      "Epoch 9/1000\n",
      "112/112 [==============================] - 8s 73ms/step - loss: 0.2328 - acc: 0.9281 - val_loss: 2.8933 - val_acc: 0.4625\n",
      "Epoch 10/1000\n",
      "112/112 [==============================] - 8s 74ms/step - loss: 0.1674 - acc: 0.9461 - val_loss: 3.0322 - val_acc: 0.4938\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (128,128)\n",
    "INPUT_SHAPE = (128,128,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(7,7), \n",
    "                                     strides=(2,2),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))           \n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))        \n",
    "    model.add(tf.keras.layers.Dropout(0.3)) \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(64, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.3))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        #optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        optimizer=tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "112/112 [==============================] - 64s 570ms/step - loss: 2.8324 - acc: 0.1256 - val_loss: 2.3976 - val_acc: 0.3312\n",
      "Epoch 2/1000\n",
      "112/112 [==============================] - 63s 560ms/step - loss: 2.3354 - acc: 0.2937 - val_loss: 1.9252 - val_acc: 0.4875\n",
      "Epoch 3/1000\n",
      "112/112 [==============================] - 59s 523ms/step - loss: 1.9952 - acc: 0.3981 - val_loss: 1.7035 - val_acc: 0.5500\n",
      "Epoch 4/1000\n",
      "112/112 [==============================] - 62s 555ms/step - loss: 1.7516 - acc: 0.4651 - val_loss: 1.5771 - val_acc: 0.5563\n",
      "Epoch 5/1000\n",
      "112/112 [==============================] - 60s 536ms/step - loss: 1.6271 - acc: 0.5090 - val_loss: 1.4593 - val_acc: 0.5813\n",
      "Epoch 6/1000\n",
      "112/112 [==============================] - 59s 531ms/step - loss: 1.4718 - acc: 0.5449 - val_loss: 1.4329 - val_acc: 0.5563\n",
      "Epoch 7/1000\n",
      "112/112 [==============================] - 58s 521ms/step - loss: 1.3701 - acc: 0.5722 - val_loss: 1.3455 - val_acc: 0.6188\n",
      "Epoch 8/1000\n",
      "112/112 [==============================] - 59s 527ms/step - loss: 1.2741 - acc: 0.5936 - val_loss: 1.2706 - val_acc: 0.6500\n",
      "Epoch 9/1000\n",
      "112/112 [==============================] - 60s 538ms/step - loss: 1.1910 - acc: 0.6242 - val_loss: 1.2225 - val_acc: 0.6500\n",
      "Epoch 10/1000\n",
      "112/112 [==============================] - 60s 532ms/step - loss: 1.1854 - acc: 0.6162 - val_loss: 1.2796 - val_acc: 0.6438\n",
      "Epoch 11/1000\n",
      "112/112 [==============================] - 61s 542ms/step - loss: 1.0856 - acc: 0.6536 - val_loss: 1.2673 - val_acc: 0.6250\n",
      "Epoch 12/1000\n",
      "112/112 [==============================] - 60s 535ms/step - loss: 1.0289 - acc: 0.6628 - val_loss: 1.2457 - val_acc: 0.6875\n",
      "Epoch 13/1000\n",
      "112/112 [==============================] - 59s 528ms/step - loss: 0.9827 - acc: 0.6790 - val_loss: 1.2130 - val_acc: 0.6687\n",
      "Epoch 14/1000\n",
      "112/112 [==============================] - 59s 527ms/step - loss: 0.9587 - acc: 0.6879 - val_loss: 1.2806 - val_acc: 0.6312\n",
      "Epoch 15/1000\n",
      "112/112 [==============================] - 56s 497ms/step - loss: 0.8948 - acc: 0.6954 - val_loss: 1.2165 - val_acc: 0.6687\n",
      "Epoch 16/1000\n",
      "112/112 [==============================] - 57s 510ms/step - loss: 0.8646 - acc: 0.7098 - val_loss: 1.2580 - val_acc: 0.6312\n",
      "Epoch 17/1000\n",
      "112/112 [==============================] - 58s 518ms/step - loss: 0.8037 - acc: 0.7301 - val_loss: 1.2696 - val_acc: 0.6750\n",
      "Epoch 18/1000\n",
      "112/112 [==============================] - 55s 490ms/step - loss: 0.7622 - acc: 0.7479 - val_loss: 1.3303 - val_acc: 0.6750\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (128,128)\n",
    "INPUT_SHAPE = (128,128,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(7,7), \n",
    "                                     strides=(2,2),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))           \n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(3,3))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(3,3),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))        \n",
    "    model.add(tf.keras.layers.Dropout(0.3)) \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(64, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.3))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        #optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        optimizer=tf.keras.optimizers.Adam(), \n",
    "#         optimizer=tf.keras.optimizers.SGD(lr=0.01),\n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (128,128)\n",
    "INPUT_SHAPE = (128,128,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(7,7), \n",
    "                                     strides=(2,2),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))           \n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(3,3))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))        \n",
    "    model.add(tf.keras.layers.Dropout(0.3)) \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(64, activation=None))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.3))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        #optimizer=\"rmsprop\", # tf.keras.optimizers.Adam(), \n",
    "        optimizer=tf.keras.optimizers.Adam(), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3605 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "54/54 [==============================] - 42s 782ms/step - loss: 2.9771 - acc: 0.0895 - val_loss: 2.5263 - val_acc: 0.2424\n",
      "Epoch 2/1000\n",
      "54/54 [==============================] - 41s 759ms/step - loss: 2.5481 - acc: 0.2166 - val_loss: 2.0953 - val_acc: 0.3939\n",
      "Epoch 3/1000\n",
      "54/54 [==============================] - 41s 760ms/step - loss: 2.2701 - acc: 0.3002 - val_loss: 1.8846 - val_acc: 0.4621\n",
      "Epoch 4/1000\n",
      "54/54 [==============================] - 41s 758ms/step - loss: 2.0833 - acc: 0.3640 - val_loss: 1.8568 - val_acc: 0.4545\n",
      "Epoch 5/1000\n",
      "54/54 [==============================] - 40s 735ms/step - loss: 1.9221 - acc: 0.4128 - val_loss: 1.6674 - val_acc: 0.5379\n",
      "Epoch 6/1000\n",
      "54/54 [==============================] - 41s 759ms/step - loss: 1.8353 - acc: 0.4408 - val_loss: 1.6373 - val_acc: 0.4924\n",
      "Epoch 7/1000\n",
      "54/54 [==============================] - 41s 755ms/step - loss: 1.7152 - acc: 0.4780 - val_loss: 1.5710 - val_acc: 0.5152\n",
      "Epoch 8/1000\n",
      "54/54 [==============================] - 41s 767ms/step - loss: 1.5917 - acc: 0.5045 - val_loss: 1.4628 - val_acc: 0.6061\n",
      "Epoch 9/1000\n",
      "54/54 [==============================] - 39s 718ms/step - loss: 1.5411 - acc: 0.5144 - val_loss: 1.4347 - val_acc: 0.5682\n",
      "Epoch 10/1000\n",
      "54/54 [==============================] - 41s 752ms/step - loss: 1.4585 - acc: 0.5397 - val_loss: 1.4085 - val_acc: 0.5682\n",
      "Epoch 11/1000\n",
      "54/54 [==============================] - 41s 758ms/step - loss: 1.3898 - acc: 0.5674 - val_loss: 1.3834 - val_acc: 0.5909\n",
      "Epoch 12/1000\n",
      "54/54 [==============================] - 42s 774ms/step - loss: 1.2988 - acc: 0.5833 - val_loss: 1.3657 - val_acc: 0.6212\n",
      "Epoch 13/1000\n",
      "54/54 [==============================] - 42s 776ms/step - loss: 1.2360 - acc: 0.6043 - val_loss: 1.3684 - val_acc: 0.6061\n",
      "Epoch 14/1000\n",
      "54/54 [==============================] - 40s 745ms/step - loss: 1.1875 - acc: 0.6226 - val_loss: 1.3355 - val_acc: 0.5909\n",
      "Epoch 15/1000\n",
      "54/54 [==============================] - 40s 738ms/step - loss: 1.1258 - acc: 0.6453 - val_loss: 1.3930 - val_acc: 0.6136\n",
      "Epoch 16/1000\n",
      "54/54 [==============================] - 41s 762ms/step - loss: 1.1063 - acc: 0.6497 - val_loss: 1.3484 - val_acc: 0.6136\n",
      "Epoch 17/1000\n",
      "54/54 [==============================] - 42s 771ms/step - loss: 1.0302 - acc: 0.6689 - val_loss: 1.3569 - val_acc: 0.5985\n",
      "Epoch 18/1000\n",
      "54/54 [==============================] - 40s 749ms/step - loss: 1.0134 - acc: 0.6742 - val_loss: 1.3595 - val_acc: 0.6136\n",
      "Epoch 19/1000\n",
      "54/54 [==============================] - 41s 764ms/step - loss: 0.9838 - acc: 0.6805 - val_loss: 1.4294 - val_acc: 0.6439\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (128,128)\n",
    "INPUT_SHAPE = (128,128,1)\n",
    "NUM_CLASSES = 22\n",
    "\n",
    "EPOCHS = 1000 \n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential() \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(7,7), \n",
    "                                     strides=(2,2),  \n",
    "                                     padding='same',                                       \n",
    "                                     activation=None, \n",
    "                                     input_shape=INPUT_SHAPE, \n",
    "                                     name='l1'))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))           \n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None, \n",
    "                                     name='l2'))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None, \n",
    "                                     name='l3'))            \n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                                     kernel_size=(5,5),\n",
    "                                     strides=(1,1),  \n",
    "                                     padding='same', \n",
    "                                     activation=None, \n",
    "                                     name='l4'))        \n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))        \n",
    "    model.add(tf.keras.layers.Dropout(0.3)) \n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())      \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(64, activation=None, name='l5'))    \n",
    "    model.add(tf.keras.layers.Activation('relu'))    \n",
    "    model.add(tf.keras.layers.Dropout(0.3))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation=None, name='l6'))\n",
    "    model.add(tf.keras.layers.Activation('softmax', name=\"output\"))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=tf.keras.optimizers.SGD(lr=0.01), \n",
    "        metrics=['accuracy']) \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "l1 (Conv2D)                  (None, 64, 64, 32)        1600      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "l2 (Conv2D)                  (None, 64, 64, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "l3 (Conv2D)                  (None, 32, 32, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "l4 (Conv2D)                  (None, 16, 16, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "l5 (Dense)                   (None, 64)                131136    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "l6 (Dense)                   (None, 22)                1430      \n",
      "_________________________________________________________________\n",
      "output (Activation)          (None, 22)                0         \n",
      "=================================================================\n",
      "Total params: 211,062\n",
      "Trainable params: 211,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2844 images belonging to 22 classes.\n",
      "Found 176 images belonging to 22 classes.\n",
      "Epoch 1/1000\n",
      "711/711 [==============================] - 44s 62ms/step - loss: 3.0878 - acc: 0.0489 - val_loss: 3.0585 - val_acc: 0.1023\n",
      "Epoch 2/1000\n",
      "711/711 [==============================] - 43s 61ms/step - loss: 2.8799 - acc: 0.1213 - val_loss: 2.5609 - val_acc: 0.2557\n",
      "Epoch 3/1000\n",
      "711/711 [==============================] - 44s 62ms/step - loss: 2.5441 - acc: 0.2201 - val_loss: 2.2478 - val_acc: 0.3523\n",
      "Epoch 4/1000\n",
      "711/711 [==============================] - 35s 49ms/step - loss: 2.2611 - acc: 0.3119 - val_loss: 1.9418 - val_acc: 0.4489\n",
      "Epoch 5/1000\n",
      "711/711 [==============================] - 33s 47ms/step - loss: 2.0837 - acc: 0.3625 - val_loss: 1.7817 - val_acc: 0.5114\n",
      "Epoch 6/1000\n",
      "711/711 [==============================] - 34s 47ms/step - loss: 1.9059 - acc: 0.4153 - val_loss: 1.7432 - val_acc: 0.5114\n",
      "Epoch 7/1000\n",
      "711/711 [==============================] - 33s 47ms/step - loss: 1.7598 - acc: 0.4508 - val_loss: 1.6452 - val_acc: 0.5398\n",
      "Epoch 8/1000\n",
      "711/711 [==============================] - 33s 47ms/step - loss: 1.6574 - acc: 0.4838 - val_loss: 1.5648 - val_acc: 0.5966\n",
      "Epoch 9/1000\n",
      "711/711 [==============================] - 34s 48ms/step - loss: 1.5632 - acc: 0.5165 - val_loss: 1.4570 - val_acc: 0.5568\n",
      "Epoch 10/1000\n",
      "711/711 [==============================] - 33s 47ms/step - loss: 1.4613 - acc: 0.5383 - val_loss: 1.4872 - val_acc: 0.5284\n",
      "Epoch 11/1000\n",
      "711/711 [==============================] - 34s 48ms/step - loss: 1.3638 - acc: 0.5731 - val_loss: 1.3602 - val_acc: 0.5966\n",
      "Epoch 12/1000\n",
      "711/711 [==============================] - 33s 46ms/step - loss: 1.3111 - acc: 0.5865 - val_loss: 1.4040 - val_acc: 0.5852\n",
      "Epoch 13/1000\n",
      "711/711 [==============================] - 36s 50ms/step - loss: 1.2256 - acc: 0.6065 - val_loss: 1.2689 - val_acc: 0.6080\n",
      "Epoch 14/1000\n",
      "711/711 [==============================] - 35s 49ms/step - loss: 1.1633 - acc: 0.6333 - val_loss: 1.3908 - val_acc: 0.6250\n",
      "Epoch 15/1000\n",
      "711/711 [==============================] - 34s 48ms/step - loss: 1.1321 - acc: 0.6382 - val_loss: 1.3872 - val_acc: 0.5909\n",
      "Epoch 16/1000\n",
      "711/711 [==============================] - 38s 53ms/step - loss: 1.0575 - acc: 0.6554 - val_loss: 1.2929 - val_acc: 0.6307\n",
      "Epoch 17/1000\n",
      "711/711 [==============================] - 40s 57ms/step - loss: 1.0104 - acc: 0.6733 - val_loss: 1.3102 - val_acc: 0.6136\n",
      "Epoch 18/1000\n",
      "711/711 [==============================] - 41s 58ms/step - loss: 0.9513 - acc: 0.6916 - val_loss: 1.2766 - val_acc: 0.6364\n"
     ]
    }
   ],
   "source": [
    "history, model = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 176 images belonging to 22 classes.\n",
      "('Test loss:', 1.2765541143122723)\n",
      "('Test accuracy:', 0.6363636363636364)\n"
     ]
    }
   ],
   "source": [
    "score = validate_model(model)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from keras.models import model_from_json\n",
    "\n",
    "with open('sketch_cnn.json', 'w') as f:\n",
    "    json_obj = json.loads(model.to_json())\n",
    "    json.dump(json_obj, f)\n",
    "\n",
    "model.save_weights('sketch_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of each entry in array is given by -dataType. The number of entries is equal to:\n",
    "\n",
    "inputFeatureChannels outputFeatureChannels kernelHeight kernelWidth*\n",
    "\n",
    "The layout of filter weight is as a 4D tensor (array) weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ inputChannels / groups ]\n",
    "\n",
    "Note: For binary-convolutions the layout of the weights are: weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ floor((inputChannels/groups)+31) / 32 ] with each 32 sub input feature channel index specified in machine byte order, so that for example the 13th feature channel bit can be extracted using bitmask = (1U << 13)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_conv_weights(name, wts_coef, bias_coef):\n",
    "    print(\"Exporting weights for {}\\n\\t{}\\n\\t{}\".format(name, \n",
    "          os.path.join('exports', \"{}_conv.data\".format(name)), \n",
    "          os.path.join('exports', \"{}_bias.data\".format(name))))\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # [kernel_width, kernel_height, input_feature_channels, output_feature_channels]\n",
    "    print(\"\\tOriginal weights shape {}\".format(wts_coef.shape))\n",
    "    if bias_coef is not None:\n",
    "        # [output_feature_channels]\n",
    "        print(\"\\tOriginal bias shape {}\".format(bias_coef.shape))\n",
    "    \n",
    "    # [output_feature_channels, kernel_width, kernel_height, input_feature_channels]\n",
    "    wts_coef = wts_coef.transpose(3, 0, 1, 2)\n",
    "    print(\"\\tReshaped weights shape {}\".format(wts_coef.shape))    \n",
    "    wts_coef.tofile(os.path.join('exports', \"{}_conv.data\".format(name)))    \n",
    "    \n",
    "    if bias_coef is not None:\n",
    "        bias_coef = np.squeeze(bias_coef)\n",
    "        print(\"\\tReshaped bias_coef shape {}\".format(bias_coef.shape))    \n",
    "        bias_coef.tofile(os.path.join('exports', \"{}_bias.data\".format(name)))    \n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dense_weights(name, wts_coef, bias_coef, kernel_width, kernel_height):\n",
    "    \"\"\"\n",
    "    A fully connected layer in a Convolutional Neural Network (CNN) is one where every input channel is connected \n",
    "    to every output channel. The kernel width is equal to the width of the source image, and the \n",
    "    kernel height is equal to the height of the source image. The width and height of the output is 1 x 1.\n",
    "    \n",
    "    A fully connected layer takes an MPSImage object with dimensions \n",
    "    source.width x source.height x Ni, convolves it with Weights[No][source.width][source.height][Ni], \n",
    "    and produces a 1 x 1 x No output.\n",
    "    \n",
    "    Thus, the following conditions must be true:\n",
    "    - kernelWidth == source.width\n",
    "    - kernelHeight == source.height\n",
    "    - clipRect.size.width == 1\n",
    "    - clipRect.size.height == 1\n",
    "    \n",
    "    You can think of a fully connected layer as a matrix multiplication where the image is \n",
    "    flattened into a vector of length source.width*source.height*Ni, and the weights are arranged in a \n",
    "    matrix of dimension No x (source.width*source.height*Ni) to produce an output vector of length No.\n",
    "    \n",
    "    The value of the strideInPixelsX, strideInPixelsY, and groups properties must be 1. \n",
    "    The offset property is not applicable and it is ignored. Because the clip rectangle is \n",
    "    clamped to the destination image bounds, if the destination is 1 x 1, you do not need to set the \n",
    "    clipRect property.\n",
    "    \"\"\"\n",
    "    print(\"Exporting weights for {}\\n\\t{}\\n\\t{}\".format(name, \n",
    "          os.path.join('exports', \"{}_conv.data\".format(name)), \n",
    "          os.path.join('exports', \"{}_bias.data\".format(name))))\n",
    "        \n",
    "    input_feature_channels = int(wts_coef.shape[0] / kernel_width / kernel_height) \n",
    "    output_feature_channels = wts_coef.shape[-1]            \n",
    "    \n",
    "    # [kernel_width, kernel_height, input_feature_channels, output_feature_channels]\n",
    "    print(\"\\tOriginal weights shape {}\".format(wts_coef.shape))\n",
    "    \n",
    "    #wts_coef = np.reshape(wts_coef, [kernel_width, kernel_height, input_feature_channels, output_feature_channels])    \n",
    "    wts_coef = np.reshape(wts_coef, [kernel_width, kernel_height, -1, output_feature_channels])    \n",
    "        \n",
    "    if bias_coef is not None:\n",
    "        # [output_feature_channels]\n",
    "        print(\"\\tOriginal bias shape {}\".format(bias_coef.shape))\n",
    "    \n",
    "    # [output_feature_channels, kernel_width, kernel_height, input_feature_channels]\n",
    "    wts_coef = wts_coef.transpose(3, 0, 1, 2)\n",
    "    print(\"\\tReshaped weights shape {}\".format(wts_coef.shape))    \n",
    "    wts_coef.tofile(os.path.join('exports', \"{}_conv.data\".format(name)))    \n",
    "    \n",
    "    if bias_coef is not None:\n",
    "        bias_coef = np.squeeze(bias_coef)\n",
    "        print(\"\\tReshaped bias_coef shape {}\".format(bias_coef.shape))    \n",
    "        bias_coef.tofile(os.path.join('exports', \"{}_bias.data\".format(name)))   \n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting weights for l1\n",
      "\texports/l1_conv.data\n",
      "\texports/l1_bias.data\n",
      "\n",
      "\n",
      "\tOriginal weights shape (7, 7, 1, 32)\n",
      "\tOriginal bias shape (32,)\n",
      "\tReshaped weights shape (32, 7, 7, 1)\n",
      "\tReshaped bias_coef shape (32,)\n",
      "\n",
      "\n",
      "Exporting weights for l2\n",
      "\texports/l2_conv.data\n",
      "\texports/l2_bias.data\n",
      "\n",
      "\n",
      "\tOriginal weights shape (5, 5, 32, 32)\n",
      "\tOriginal bias shape (32,)\n",
      "\tReshaped weights shape (32, 5, 5, 32)\n",
      "\tReshaped bias_coef shape (32,)\n",
      "\n",
      "\n",
      "Exporting weights for l3\n",
      "\texports/l3_conv.data\n",
      "\texports/l3_bias.data\n",
      "\n",
      "\n",
      "\tOriginal weights shape (5, 5, 32, 32)\n",
      "\tOriginal bias shape (32,)\n",
      "\tReshaped weights shape (32, 5, 5, 32)\n",
      "\tReshaped bias_coef shape (32,)\n",
      "\n",
      "\n",
      "Exporting weights for l4\n",
      "\texports/l4_conv.data\n",
      "\texports/l4_bias.data\n",
      "\n",
      "\n",
      "\tOriginal weights shape (5, 5, 32, 32)\n",
      "\tOriginal bias shape (32,)\n",
      "\tReshaped weights shape (32, 5, 5, 32)\n",
      "\tReshaped bias_coef shape (32,)\n",
      "\n",
      "\n",
      "Exporting weights for l5\n",
      "\texports/l5_conv.data\n",
      "\texports/l5_bias.data\n",
      "\tOriginal weights shape (2048, 64)\n",
      "\tOriginal bias shape (64,)\n",
      "\tReshaped weights shape (64, 8, 8, 32)\n",
      "\tReshaped bias_coef shape (64,)\n",
      "\n",
      "\n",
      "Exporting weights for l6\n",
      "\texports/l6_conv.data\n",
      "\texports/l6_bias.data\n",
      "\tOriginal weights shape (64, 22)\n",
      "\tOriginal bias shape (22,)\n",
      "\tReshaped weights shape (22, 1, 1, 64)\n",
      "\tReshaped bias_coef shape (22,)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### flatted_input_kernel_width = None\n",
    "flatted_input_kernel_height = None\n",
    "\n",
    "for layer in model.layers:        \n",
    "    if \"flatten\" in layer.name:\n",
    "        flatted_input_kernel_width = layer.input_shape[1] \n",
    "        flatted_input_kernel_height = layer.input_shape[2] \n",
    "        \n",
    "    if len(layer.get_weights()) > 0:        \n",
    "        name = layer.name\n",
    "        wts = layer.get_weights()\n",
    "        \n",
    "        if name in ['l1', 'l2', 'l3', 'l4']:\n",
    "            export_conv_weights(layer.name, wts[0], wts[1] if len(wts) == 2 else None)        \n",
    "        elif name in ['l5', 'l6']:\n",
    "            export_dense_weights(layer.name, wts[0], wts[1] if len(wts) == 2 else None, \n",
    "                                flatted_input_kernel_width, flatted_input_kernel_height)        \n",
    "            # after the initial pass (from cnn to fcn); flattern the kernel down to 1x1 \n",
    "            # i.e. update the flatted_input_kernel_DIM to have the kernel width and height of 1 \n",
    "            flatted_input_kernel_width, flatted_input_kernel_height = 1, 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect models initlised weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = tmp_model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.05900718,  0.04470242, -0.04612648, ...,  0.04839333,\n",
       "           0.00087062, -0.02341526]],\n",
       "\n",
       "        [[-0.0533563 , -0.0567241 , -0.00595827, ...,  0.03955685,\n",
       "          -0.00030188,  0.0498188 ]],\n",
       "\n",
       "        [[ 0.01904735,  0.0105496 , -0.01652239, ...,  0.04525235,\n",
       "          -0.03117971, -0.02302255]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0604782 ,  0.03106899, -0.03467188, ..., -0.04902239,\n",
       "          -0.04092151, -0.03075201]],\n",
       "\n",
       "        [[ 0.05375518,  0.0057967 , -0.03389849, ..., -0.04101713,\n",
       "           0.02911748,  0.00312194]],\n",
       "\n",
       "        [[-0.0527735 ,  0.01116358, -0.05607419, ..., -0.03250823,\n",
       "          -0.03193898, -0.05470339]]],\n",
       "\n",
       "\n",
       "       [[[ 0.06032657, -0.04008624, -0.0504399 , ...,  0.01788989,\n",
       "          -0.00406332, -0.04738389]],\n",
       "\n",
       "        [[-0.00438863,  0.02607093, -0.04305125, ...,  0.03807356,\n",
       "          -0.00495933, -0.05969848]],\n",
       "\n",
       "        [[-0.04564478, -0.0590722 , -0.02759542, ...,  0.05074384,\n",
       "           0.00731297,  0.02065421]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0497763 , -0.03966071,  0.05693811, ...,  0.04052863,\n",
       "           0.00421954,  0.00623414]],\n",
       "\n",
       "        [[ 0.00531884, -0.05935871, -0.03643483, ..., -0.0247964 ,\n",
       "          -0.04600313, -0.00892709]],\n",
       "\n",
       "        [[-0.0435782 ,  0.01014613, -0.01414225, ..., -0.02974917,\n",
       "           0.05855484, -0.02421805]]],\n",
       "\n",
       "\n",
       "       [[[-0.00643406,  0.02631482,  0.04848861, ...,  0.0189506 ,\n",
       "           0.02979182,  0.01203327]],\n",
       "\n",
       "        [[-0.02293264, -0.00984645,  0.04209717, ...,  0.01242769,\n",
       "           0.03796022, -0.04318016]],\n",
       "\n",
       "        [[ 0.02478225,  0.04328015, -0.0591835 , ...,  0.02451954,\n",
       "          -0.019086  , -0.00023597]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.04542958,  0.01808859, -0.00078138, ...,  0.04085543,\n",
       "          -0.03705017,  0.03874001]],\n",
       "\n",
       "        [[-0.02727747,  0.05227448,  0.03135087, ..., -0.0391089 ,\n",
       "           0.01774814,  0.04432859]],\n",
       "\n",
       "        [[-0.01698247, -0.0151409 ,  0.01085177, ..., -0.00642854,\n",
       "           0.03727588, -0.03454328]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[-0.0194681 , -0.02977695, -0.05263037, ..., -0.05123391,\n",
       "           0.01303658, -0.03935322]],\n",
       "\n",
       "        [[ 0.02172975, -0.023615  , -0.0552029 , ...,  0.00100463,\n",
       "          -0.00583412, -0.03358566]],\n",
       "\n",
       "        [[ 0.04969921,  0.04977769, -0.00985966, ..., -0.03271912,\n",
       "           0.05990204,  0.00325165]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.06031733,  0.04740552,  0.01413729, ..., -0.04867595,\n",
       "           0.0194311 , -0.06007436]],\n",
       "\n",
       "        [[ 0.015405  ,  0.02950485,  0.05211481, ..., -0.02089252,\n",
       "           0.02355288, -0.02421035]],\n",
       "\n",
       "        [[-0.05791255, -0.02749237, -0.03607705, ...,  0.00237253,\n",
       "           0.05522646, -0.0222028 ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.02850026, -0.05391676,  0.03717115, ...,  0.01418829,\n",
       "           0.00843013,  0.0182027 ]],\n",
       "\n",
       "        [[ 0.01771839, -0.02665128,  0.04301024, ...,  0.03438067,\n",
       "           0.0484721 ,  0.00488594]],\n",
       "\n",
       "        [[-0.00953857, -0.00294486,  0.04208273, ...,  0.01473499,\n",
       "          -0.04761518, -0.02577032]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0128883 ,  0.03866043, -0.01319444, ..., -0.01844249,\n",
       "          -0.01001221,  0.0055006 ]],\n",
       "\n",
       "        [[ 0.03279812,  0.01104588, -0.02489169, ...,  0.00202766,\n",
       "          -0.01288681,  0.02360285]],\n",
       "\n",
       "        [[ 0.05674512, -0.01381408,  0.02071742, ..., -0.02492297,\n",
       "          -0.03600576,  0.02102282]]],\n",
       "\n",
       "\n",
       "       [[[ 0.01454409, -0.00817954, -0.01981573, ..., -0.03981041,\n",
       "          -0.00776874,  0.05505373]],\n",
       "\n",
       "        [[ 0.00918332,  0.01405472, -0.02420262, ...,  0.05101806,\n",
       "          -0.00853849, -0.0069099 ]],\n",
       "\n",
       "        [[ 0.0467894 ,  0.00756985,  0.04768928, ...,  0.05780166,\n",
       "          -0.03489292,  0.0094751 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.01334198, -0.01734632,  0.00196728, ...,  0.03884536,\n",
       "          -0.002086  ,  0.02279412]],\n",
       "\n",
       "        [[-0.00432714,  0.03069824,  0.03030687, ..., -0.01084787,\n",
       "           0.01607214, -0.05199999]],\n",
       "\n",
       "        [[-0.03930941, -0.04561434,  0.00151327, ...,  0.04465786,\n",
       "           0.03928101, -0.00102875]]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4swift27",
   "language": "python",
   "name": "dl4swift27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
